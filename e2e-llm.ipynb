{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set Up","metadata":{"_uuid":"6dde6acf-6d75-41c2-a2a6-571518c66ba3","_cell_guid":"cdf3ee35-a08a-43ad-baeb-66f7e4cbb7ad","trusted":true}},{"cell_type":"code","source":"%reset","metadata":{"_uuid":"a8632913-1586-488c-bf86-bb1153027224","_cell_guid":"4fd5ba6e-9592-4717-b5dc-562968ed6617","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -a","metadata":{"_uuid":"ad12e78a-f5ea-49ff-8560-d5b8b90881b8","_cell_guid":"eaff0ecd-f3d3-4466-aec7-eabc85792aac","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# from datetime import datetime\n# from prettytable import PrettyTable\n\n# def find_notebooks(start_path='.'):\n#     notebooks = []\n#     for root, dirs, files in os.walk(start_path):\n#         for file in files:\n#             if file.endswith('.ipynb'):\n#                 full_path = os.path.join(root, file)\n#                 mod_time = os.path.getmtime(full_path)\n#                 mod_date = datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S')\n#                 notebooks.append((file, full_path, mod_date))\n#     return notebooks\n\n# def create_notebook_table(notebooks):\n#     table = PrettyTable()\n#     table.field_names = [\"Name\", \"Location\", \"Last Modified\"]\n#     table.align[\"Name\"] = \"l\"\n#     table.align[\"Location\"] = \"l\"\n#     table.align[\"Last Modified\"] = \"l\"\n#     for notebook in notebooks:\n#         table.add_row(notebook)\n#     return table\n\n# # Find all notebooks\n# notebooks = find_notebooks('/')\n\n# # Create and print the table\n# table = create_notebook_table(notebooks)\n# print(table)\n\n# # Print total count\n# print(f\"\\nTotal notebooks found: {len(notebooks)}\")","metadata":{"_uuid":"357393e6-a20f-4dbe-a61b-c98dfcde6396","_cell_guid":"3407f15c-01b3-4782-8dad-4cfae6057cbe","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd \"/opt/conda/pkgs/conda-4.12.0-py310h06a4308_0/info/test/tests/conda_env/support/\"\n!pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python --version","metadata":{"_uuid":"7e89b17e-6958-41b7-bd94-ddc300345426","_cell_guid":"67a88632-a1d0-48e9-82d9-bc47a73d03a5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git config --global user.email \"mdrafat.siddiqui@outlook.com\"","metadata":{"_uuid":"895ede56-32aa-4997-83f9-bed60972ceab","_cell_guid":"170e8502-53ae-4b33-a88f-35ec31b16923","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Backup the current list of files\n!ls > current_files.txt\n\n# Update .gitignore to only ignore existing files except the notebooks we want to keep\n!echo \"\" > .gitignore\n!echo \"# Ignore all existing files\" >> .gitignore\n!for file in $(cat current_files.txt); do if [[ \"$file\" != \"e2e-llm.ipynb\" && \"$file\" != \"notebook.ipynb\" && \"$file\" != \"notebook_with_env.ipynb\" && \"$file\" != \".gitignore\" ]]; then echo \"/$file\" >> .gitignore; fi; done\n\n# Add exceptions for the notebooks we want to keep\n!echo \"# Keep these notebooks\" >> .gitignore\n!echo \"!e2e-llm.ipynb\" >> .gitignore\n!echo \"!notebook.ipynb\" >> .gitignore\n!echo \"!notebook_with_env.ipynb\" >> .gitignore\n\n# Display the contents of .gitignore\nprint(\"Contents of .gitignore:\")\n!cat .gitignore\n\n# Remove the temporary file\n!rm current_files.txt\n\n# Add .gitignore to the repository\n!git add .gitignore\n!git commit -m \"Update .gitignore to ignore existing files but allow new ones\"\n\n# Push changes\n!git push origin main\n\nprint(\"\\nUpdated .gitignore to ignore existing files but allow new ones.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf .git\n!git init\n!git remote add origin https://rafatsiddiqui9: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@github.com/rafatsiddiqui9/e2e-llm-backup\n!git add .\n!git commit -m \"Connecting notebook folder from Kaggle\"\n!git fetch origin main\n!git checkout -b main\n!git pull --rebase origin main \n!git push -u origin main","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Magic Functions for Github","metadata":{"_uuid":"2122a74f-8729-40b6-af9a-cef2810e8508","_cell_guid":"8b375b53-3c74-4ff6-8edc-b661e7db2403","trusted":true}},{"cell_type":"code","source":"from IPython.core.magic import register_cell_magic\nimport subprocess\nimport re\nfrom datetime import datetime\nimport pytz\nimport threading\nimport time\n\ndef delete_branch(branch_name):\n    try:\n        # Delete the branch locally\n        subprocess.run([\"git\", \"branch\", \"-D\", branch_name], check=True)\n        # Delete the branch remotely\n        subprocess.run([\"git\", \"push\", \"origin\", \"--delete\", branch_name], check=True)\n        print(f\"Deleted branch '{branch_name}'\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error deleting branch: {e}\")\n\ndef save_and_push(is_auto=False):\n    # Get current Indian Standard Time\n    ist = datetime.now(pytz.timezone('Asia/Kolkata'))\n    branch_name = ist.strftime(\"%d-%b-%Y-%H%M-IST\")\n    if is_auto:\n        branch_name += \"-autosave\"\n    \n    try:\n        # If it's an auto-save, delete the previous auto-save branch\n        if is_auto:\n            subprocess.run([\"git\", \"fetch\", \"--all\"], check=True)\n            result = subprocess.run([\"git\", \"branch\", \"-r\"], capture_output=True, text=True, check=True)\n            for branch in result.stdout.split('\\n'):\n                if \"-autosave\" in branch:\n                    delete_branch(branch.strip().split('/')[-1])\n        \n        subprocess.run([\"git\", \"checkout\", \"-b\", branch_name], check=True)\n        subprocess.run([\"git\", \"add\", \".\"], check=True)\n        commit_message = f\"Auto-save at {branch_name}\" if is_auto else f\"Manual save at {branch_name}\"\n        subprocess.run([\"git\", \"commit\", \"-m\", commit_message], check=True)\n        subprocess.run([\"git\", \"push\", \"origin\", branch_name], check=True)\n        print(f\"Changes pushed to new branch '{branch_name}' successfully.\")\n    except subprocess.CalledProcessError as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        subprocess.run([\"git\", \"checkout\", \"main\"], check=True)\n\ndef auto_save_thread():\n    while True:\n        time.sleep(120)  # Wait for 2 minutes\n        save_and_push(is_auto=True)\n\n# Start the auto-save thread\nthreading.Thread(target=auto_save_thread, daemon=True).start()\n\n@register_cell_magic\ndef ap1(line, cell):\n    exec(cell, globals())\n    save_and_push()\n\n@register_cell_magic\ndef ap2(line, cell):\n    exec(cell, globals())\n    save_and_push()\n\nip = get_ipython()\nip.register_magic_function(ap1, 'cell')\nip.register_magic_function(ap2, 'cell')\n\nprint(\"Auto-save and push system set up. It will run every 2 minutes.\")","metadata":{"_uuid":"21c533c5-5515-446f-87da-63dc77b81ef2","_cell_guid":"7376188c-4c72-494c-b5b4-17dad94b2b17","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Magic Function for Visual Feedback","metadata":{"_uuid":"2373f2a4-e21e-49ea-bd56-8d396289174f","_cell_guid":"66850348-d8cc-4044-b134-2863b32c282b","trusted":true}},{"cell_type":"code","source":"from IPython.display import display, HTML\nfrom IPython import get_ipython\n\ndef display_feedback(success):\n    color = \"green\" if success else \"red\"\n    if success:\n        display(HTML(f\"<div style='width:20px;height:20px;background-color:{color};border-radius:50%;'></div>\"))\n    else:\n        display(HTML(f\"<div style='width:100px;height:100px;background-color:{color};border-radius:50%;'></div>\"))\n\ndef visual_feedback_after_execution(result):\n    success = not (result.error_before_exec or result.error_in_exec)\n    display_feedback(success)\n\n# Get the current IPython instance\nip = get_ipython()\n\n# Remove the audio hook\nfor hook in ip.events.callbacks['post_run_cell']:\n    if 'play_audio_after_execution' in str(hook):\n        ip.events.callbacks['post_run_cell'].remove(hook)\n# Register the visual feedback hook\nip.events.register('post_run_cell', visual_feedback_after_execution)\n\nprint(\"Visual feedback system set up.\")","metadata":{"_uuid":"d711fe19-2fcc-438e-bb93-898b3e018eb8","_cell_guid":"2e6617f5-37a8-4bb4-97d8-7c05938cce87","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Magic Function for sound","metadata":{"_uuid":"f7788163-3183-44da-b165-ce620ba69a29","_cell_guid":"0120c5d3-07a9-4ccd-953a-ed1036e1ad81","trusted":true}},{"cell_type":"code","source":"# # !pip install pydub --no-cache-dir\n\n# import requests\n\n# # Google Drive file ID\n# file_id = \"12v2RwutER9ayuPe4d4H0GQAxA9iUgdXx\"\n# url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n\n# # Send a GET request to the URL\n# response = requests.get(url)\n\n# # Save the content of the response as an MP3 file\n# with open(\"error.mp3\", \"wb\") as file:\n#     file.write(response.content)\n\n# print(\"File downloaded and saved as error.mp3\")\n\n# import requests\n\n# # Google Drive file ID\n# file_id = \"1-kfEx5SgDxB0ph71q9BhKAEB8R4qShFx\"\n# url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n\n# # Send a GET request to the URL\n# response = requests.get(url)\n\n# # Save the content of the response as an MP3 file\n# with open(\"success.mp3\", \"wb\") as file:\n#     file.write(response.content)\n\n# print(\"File downloaded and saved as success.mp3\")\n\n# from IPython.display import display, Audio\n# from IPython import get_ipython\n# from pydub import AudioSegment\n\n# # Define the post-execution hook\n# def play_audio_after_execution(result):\n#     if result.error_before_exec or result.error_in_exec:\n#         # Load and adjust volume of the error sound\n#         audio = AudioSegment.from_mp3(\"error.mp3\")\n#     else:\n#         # Load and adjust volume of the success sound\n#         audio = AudioSegment.from_mp3(\"success.mp3\")\n\n#     # Set volume to 50%\n#     audio = audio - 25 # Reduce volume by 10dB (approximately 50%)\n\n#     # Export to a temporary file and play\n#     audio.export(\"temp_output.mp3\", format=\"mp3\")\n#     display(Audio(\"temp_output.mp3\", autoplay=True))\n\n# # Get the current IPython instance\n# ip = get_ipython()\n\n# # Register the post-execution hook\n# ip.events.register('post_run_cell', play_audio_after_execution)","metadata":{"_uuid":"05e45631-3195-4735-aa05-7292683459f4","_cell_guid":"958ce95f-8e5a-49b6-9764-4e0ad83ba8b8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Magic Function for Updating Conda Libraries","metadata":{"_uuid":"3189bb7d-7edf-40c0-aa66-15ccbfb9277b","_cell_guid":"ba47ce07-600f-405f-8995-dcba93657bda","trusted":true}},{"cell_type":"code","source":"# import subprocess\n# import time\n# from tqdm import tqdm\n\n# print(\"Updating all packages in the current Conda environment...\")\n\n# # Start the conda update process\n# process = subprocess.Popen([\"conda\", \"update\", \"--all\", \"-y\"], \n#                            stdout=subprocess.PIPE, \n#                            stderr=subprocess.STDOUT,\n#                            universal_newlines=True)\n\n# # Create a progress bar\n# with tqdm(total=100, desc=\"Updating\", bar_format=\"{l_bar}{bar}\", ncols=50) as pbar:\n#     # Read the output line by line\n#     for line in iter(process.stdout.readline, ''):\n#         if line:\n#             # Update progress bar\n#             pbar.update(1)\n#             time.sleep(0.1)  # Add a small delay to make the progress visible\n        \n#         # Check if process has finished\n#         if process.poll() is not None:\n#             break\n\n# # Ensure the progress bar reaches 100%\n# pbar.update(100 - pbar.n)\n\n# # Check the return code\n# if process.returncode != 0:\n#     print(f\"Error occurred. Return code: {process.returncode}\")\n# else:\n#     print(\"Update complete!\")","metadata":{"_uuid":"10f03123-912f-4025-be10-766652d10521","_cell_guid":"309b2448-dde7-4c95-a7e6-51564babe5ac","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main Workflow","metadata":{"_uuid":"8ecef8c7-099a-4bd0-801b-9419dd4ac2a2","_cell_guid":"2644d883-3a81-4dd0-a291-e1bec6f500b6","trusted":true}},{"cell_type":"markdown","source":"## Install Libraries from SCRATCH","metadata":{"_uuid":"83895670-704c-4385-b696-6ba967539e66","_cell_guid":"34ccf1f5-6d91-423b-89fd-5006705aceb1","trusted":true}},{"cell_type":"code","source":"%%capture\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes","metadata":{"_uuid":"d941584f-4d1f-47ce-8e77-5ea067f0440e","_cell_guid":"191b58b1-8ae9-4751-ae8a-44b77a32ff2c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n\n# Install necessary libraries without dependencies\n!pip install -U mlflow datasets transformers evaluate autoawq seaborn vllm xformers triton --no-cache-dir\n\n# Check CUDA version and NVIDIA driver status\n!nvcc --version\n!nvidia-smi\n\n# Install PyTorch and related packages\n!pip install -U torch torchvision torchaudio --no-cache-dir\n\n# Set environment variables for optimized performance\n!export OMP_NUM_THREADS=1\n!export MKL_NUM_THREADS=1\n\n# Verify CUDA availability in PyTorch\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"_uuid":"d3286683-480f-48ef-8a5d-a366a59d6fd9","_cell_guid":"89a653e4-abd5-42a2-8d92-dfc44d53e954","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Install libraries from CACHE","metadata":{"_uuid":"6853cbf7-4706-49fb-b18f-f91b6f404cb4","_cell_guid":"9c8ee625-8cda-4de6-844e-28cd163921f8","trusted":true}},{"cell_type":"code","source":"# %%capture\n\n# # Install Unsloth, Xformers, and other dependencies\n# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n# \n# # Install necessary libraries without dependencies\n# !pip install --no-deps mlflow datasets transformers peft accelerate trl evaluate autoawq seaborn triton\n\n# # Check CUDA version and NVIDIA driver status\n# !nvcc --version\n# !nvidia-smi\n\n# # Install PyTorch and related packages\n# !pip install torch torchvision torchaudio\n\n# !pip install triton\n\n# # Set environment variables for optimized performance\n# !export OMP_NUM_THREADS=1\n# !export MKL_NUM_THREADS=1\n\n# # Verify CUDA availability in PyTorch\n# import torch\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# print(device)","metadata":{"_uuid":"86b9ed8d-2cb0-4ad7-96a0-2150153a9281","_cell_guid":"b90a4ef4-b983-4da1-93ff-afa3ccbb92f5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ['hf_token'] = \"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\"\n\nfrom dotenv import load_dotenv\n\nload_dotenv()  # take environment variables from .env.\n\nhf_token = os.getenv('hf_token')\n\nhf_token","metadata":{"_uuid":"d3d94e5a-f55b-41da-a853-6fd769f67166","_cell_guid":"bcf13b71-0588-42b0-8882-1c74ed567086","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Libraries","metadata":{"_uuid":"ab8c15c0-9798-4118-a76d-7640179e9a18","_cell_guid":"2fff066b-2a1a-4883-9acd-1fa3c8a8cdc0","trusted":true}},{"cell_type":"code","source":"# Import required libraries\nimport unsloth\nimport bitsandbytes\nimport mlflow\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, AutoTokenizer\nfrom unsloth import is_bfloat16_supported\nfrom trl import SFTTrainer\nimport evaluate\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport shutil","metadata":{"_uuid":"48e8a36c-b492-42ed-afc8-d569e329e1f9","_cell_guid":"19fd8865-afc1-451f-b4ca-6cdddd8b7b5b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !export HUGGINGFACE_HUB_TOKEN= hf_token\n# !export TOKENIZERS_PARALLELISM=false","metadata":{"_uuid":"69385243-0c96-4d6a-8703-5534e0b143ee","_cell_guid":"a7652c60-b59a-4fa8-998a-6bc720214378","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set up MLFlow","metadata":{"_uuid":"771d18d4-39e6-4e5e-a3a9-67da6a3d96fc","_cell_guid":"28854f2e-a905-464f-9b2c-b19d1b2f2c33","trusted":true}},{"cell_type":"code","source":"# Clear cache directory\ncache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\nif os.path.exists(cache_dir):\n    shutil.rmtree(cache_dir)\nos.makedirs(cache_dir)\n\n# Check if CUDA is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize MLflow\nmlflow.set_tracking_uri(\"file:///mnt/data/mlruns\")\nmlflow.set_experiment(\"unsloth-train\")","metadata":{"_uuid":"7bc87a22-4eda-48db-9fbe-5e4a3b590e13","_cell_guid":"0cbd9342-ba23-4629-bae3-20d929e20d61","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vanilla Unsloth Code","metadata":{"_uuid":"38737e42-f8e1-4118-97c2-3958904c492c","_cell_guid":"3a817861-b01e-4d79-9e64-778c6bf5a16a","trusted":true}},{"cell_type":"markdown","source":"### Import unsloth model","metadata":{"_uuid":"ddfc4594-aa8c-4dc0-8062-35486857bc26","_cell_guid":"f422b3b3-ac23-4ebe-be8d-249f53071c4c","trusted":true}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"_uuid":"e62828b9-c378-4759-9c20-591d089b2857","_cell_guid":"cf924597-7b47-411f-8f5e-8230022d2080","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configure with PEFT","metadata":{"_uuid":"da2fc1b9-6efe-4971-a50e-727da429c0b9","_cell_guid":"05f6ee32-a725-447c-bb23-66bc7bc98856","trusted":true}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"_uuid":"a5353351-4d51-4910-9cd5-ab5393a9587b","_cell_guid":"c4950d0b-706b-466f-abb9-64a1b6a0d958","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Frame the prompt","metadata":{"_uuid":"e69aa6e6-b368-45ed-a537-a3528b930d85","_cell_guid":"22d42177-431d-4f4f-9df5-cc2b82334c15","trusted":true}},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add aEOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"_uuid":"79a8b917-4333-404d-9c73-147faf46636c","_cell_guid":"0a1e4394-6d41-47fc-95bd-d0e16fa9b9b2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Instantiate Supervised FineTuning Trainer and train","metadata":{"_uuid":"9ea8817a-25e9-431b-b457-36beb4be634f","_cell_guid":"d1ba3af5-a203-466e-9df8-f9aaf72cf01b","trusted":true}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        num_train_epochs = 5,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 5,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"_uuid":"2fb43587-b2a1-4062-b569-df2bd470bf12","_cell_guid":"de6d55ee-64e4-4cda-8a64-6b8ebe767334","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"_uuid":"50d44a20-77c0-4bef-8798-d10bfdd9727d","_cell_guid":"8a97cdc9-e43e-4326-82b0-b15b0346742c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"_uuid":"15782fe3-61f1-466a-96ab-948b54495c41","_cell_guid":"01152a0a-be5c-4b84-b637-21023fd903fa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"_uuid":"5a6b3904-8958-4f84-9977-d4a827578d64","_cell_guid":"28ff40f6-f3e6-474d-9114-537c40015329","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference form the model -Direct Output","metadata":{"_uuid":"d95b5c45-ae3d-4d70-9ded-8208ed3fa761","_cell_guid":"bb20d78a-23c0-4012-9699-7e5a218f92df","trusted":true}},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"_uuid":"45d56516-d468-4ac2-8f67-54399ea0a35d","_cell_guid":"267e9968-dbb9-457c-aaa6-6216e7958933","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference from the model - Text Streeaming","metadata":{"_uuid":"a089d5ed-d5a7-48a4-af94-29a1b9559ebb","_cell_guid":"f43051ee-6854-4b0d-b2f3-b9d5c09a7582","trusted":true}},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence till infinity.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"_uuid":"d634b25f-0a1f-4970-9dc0-a2b86c1a10ed","_cell_guid":"ad6b8140-c9d2-4549-83fc-ba7fc9888fb6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Done')","metadata":{"_uuid":"2871d490-3ae9-4369-98ac-b49ed601269a","_cell_guid":"4d9bc797-8a30-47b7-b9ee-a3e823f9d3bb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%ap2\n\nprint()","metadata":{"_uuid":"7125c9e2-3a53-47dd-a8af-b2df8ef6ffa5","_cell_guid":"24aa5c2c-69e4-4638-abda-7623e423a0ce","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FineTune on SST5","metadata":{"_uuid":"1af516af-e71a-47b7-bdd8-c57d75e1edd2","_cell_guid":"9db6d2a4-8e68-4a7c-8811-68ee1cb221d7","trusted":true}},{"cell_type":"markdown","source":"### Review the dataset and prompt for alpaca","metadata":{"_uuid":"2454fec8-b55c-434e-94af-697499a9e76d","_cell_guid":"575c624f-bc1f-421a-8612-15a248f61726","trusted":true}},{"cell_type":"markdown","source":"#### Importing the model from scratch","metadata":{"_uuid":"05c105b3-b745-444d-a059-f342d5bd2996","_cell_guid":"a3ad4a57-26b5-4f48-9f13-51fdc5a73433","trusted":true}},{"cell_type":"code","source":"import os\n\nos.environ['hf_token'] = \"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\"\n\nfrom dotenv import load_dotenv\n\nload_dotenv()  # take environment variables from .env.\n\nhf_token = os.getenv('hf_token')\n\nhf_token\n\n# Import required libraries\nimport unsloth\nimport bitsandbytes\nimport mlflow\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, AutoTokenizer\nfrom unsloth import is_bfloat16_supported\nfrom trl import SFTTrainer\nimport evaluate\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport shutil\n\n# Clear cache directory\ncache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\nif os.path.exists(cache_dir):\n    shutil.rmtree(cache_dir)\nos.makedirs(cache_dir)\n\n# Check if CUDA is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize MLflow\nmlflow.set_tracking_uri(\"file:///mnt/data/mlruns\")\nmlflow.set_experiment(\"unsloth-train\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"_uuid":"2bc8b7f7-f78c-460e-873a-2f0f7b17fc40","_cell_guid":"9049bb78-c080-4f97-8a3e-b6aa2e0ab5f3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the Stanford Sentiment Treebank (SST) dataset\nsst_dataset = load_dataset(\"SetFit/sst5\", trust_remote_code=True)\n\ndef group_sentiments(example):\n    if example[\"label\"] == 0 or example[\"label\"] == 1:\n        return \"negative\"\n    elif example[\"label\"] == 2:\n        return \"neutral\"\n    else:\n        return \"positive\"\n\n# Map the sentiments to the three categories\nsst_dataset = sst_dataset.map(lambda x: {\"sentiment\": group_sentiments(x)})\n\nsst_dataset","metadata":{"_uuid":"a8b01cc6-f318-499f-a41d-8a81b44b7943","_cell_guid":"2859b29b-ea39-4201-8a94-bbabfaf0c3b3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"\nBelow is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the task.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\n\"\"\"","metadata":{"_uuid":"774215e6-c6d9-4704-9d81-5bbc8b7445ff","_cell_guid":"82758112-af20-4f7b-a7c0-82a48d024cb5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\n\ndef process_dataset(dataset, instruction_text):\n    def process_split(split):\n        df = pd.DataFrame(sst_dataset[split])\n        df['instruction'] = instruction_text\n        df['output'] = df['sentiment']\n        df['input'] = df['text']\n        df.drop(labels=['text', 'label', 'label_text', 'sentiment'], axis=1, inplace=True)\n        return df\n\n    processed_dict = DatasetDict({\n        split: Dataset.from_pandas(process_split(split))\n        for split in ['train', 'validation', 'test']\n    })\n\n    return processed_dict\n\n# Usage\ninstruction_text = \"\"\"Read and analyze the sentiment of the provided input.\nFirst, consider if the text is straight forward or sarcastic or containing double meaning or is expressed inversely.\nThen give a response labelling sentiment as either positive or neutral or negative or unclear.\n\"\"\"\nprocessed_dataset = process_dataset(sst_dataset, instruction_text)\nprocessed_dataset","metadata":{"_uuid":"23168526-ea30-4da7-a19b-9284e8202092","_cell_guid":"3bcf121d-951a-4ca8-b6a4-3966da59db09","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\n# dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\nprocessed_dataset_alpaca = processed_dataset.map(formatting_prompts_func, batched = True,)","metadata":{"_uuid":"787077e3-3593-4f6e-b7ce-cb91e90407d9","_cell_guid":"15c6771f-e5b6-4b70-a178-607732030708","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_dataset_alpaca","metadata":{"_uuid":"156a3816-5461-480a-afc5-a736a9a657f4","_cell_guid":"68da6333-39e5-4641-a00e-1be2a8b2037f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_dataset_alpaca['train']['text'][0]","metadata":{"_uuid":"4a33f09c-d9b8-45cf-9b8b-6a7e247236fb","_cell_guid":"af5bddbb-a8a7-46f2-9001-90737fe3a8fc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"_uuid":"e31907a0-24db-49fa-bdd3-ee8ec447493b","_cell_guid":"d5ff27d2-6056-477d-b277-7d830e58bffc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nimport mlflow\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=50,\n    learning_rate=2e-4,\n    fp16=torch.cuda.is_available() and not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=processed_dataset_alpaca['train'],\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False, # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\ntrainer_stats = trainer.train()\n\n# Log parameters and metrics\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\nmlflow.log_metric(\"final_loss\", trainer.state.log_history[-1]['train_loss'])","metadata":{"_uuid":"33bc4b99-1ea4-4e03-aac7-94ebfbc4dda9","_cell_guid":"fc5b4402-f74a-47c0-9248-02e3f75f95e1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"_uuid":"fb17d1f9-4838-452e-9692-b1a042183aa6","_cell_guid":"c82ba7c5-b2d4-4698-968d-e1b1ddb12b45","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\n\nsarcastic_statements = [\n    \"Oh great, another meeting. Just what I needed to make my day perfect.\",\n    \"Wow, you're so good at giving half the effort and still expecting full credit.\",\n    \"Yeah, because nothing says 'professional' like missing every single deadline.\",\n    \"Oh, you forgot again? Shocking.\",\n    \"Amazing, you managed to turn a simple task into a complete disaster. Bravo!\",\n    \"Oh, please, tell me more about how everyone else is wrong and you're always right.\",\n    \"Fantastic, another software update that makes everything worse.\",\n    \"Sure, let's keep doing it your way since it's worked out so brilliantly so far.\",\n    \"Oh, how convenient that you showed up right when the work is done.\",\n    \"Wow, your ability to avoid responsibility is truly inspirational.\"\n]\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\nfor i in sarcastic_statements:\n    inputs = tokenizer(\n    [\n        alpaca_prompt.format(\n            \"Identify the sentiment in the text below\", # instruction\n            i, # input\n            \"\", # output - leave this blank for generation!\n        )\n    ], return_tensors = \"pt\").to(\"cuda\")\n\n    from transformers import TextStreamer\n    text_streamer = TextStreamer(tokenizer)\n    _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"_uuid":"f11cc2ae-b389-4a9d-a6e7-c98a1c2a6047","_cell_guid":"3fa927b8-e7fb-4647-b7e3-5688186a657c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"Meta-Llama-3-8B-SST-FineTune\") # Local saving\ntokenizer.save_pretrained(\"Meta-Llama-3-8B-SST-FineTune-Tokenizer\")\nmodel.push_to_hub(\"rafatsiddiqui/Meta-Llama-3-8B-SST-FineTune\", token = hf_token) # Online saving\ntokenizer.push_to_hub(\"rafatsiddiqui/Meta-Llama-3-8B-SST-FineTune-Tokenizer\", token = hf_token) # Online saving","metadata":{"_uuid":"34266cd8-ac63-4e90-bfb5-53ddb4864f21","_cell_guid":"44a89701-46ec-4522-a4a9-dcd447701c3b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge to 16bit\nmodel.save_pretrained_merged(\"Meta-Llama-3-8B-SST-FineTune-16bit\", tokenizer, save_method = \"merged_16bit\",)\nmodel.push_to_hub_merged(\"rafatsiddiqui/Meta-Llama-3-8B-SST-FineTune-16bit\", tokenizer, save_method = \"merged_16bit\", token = hf_token)\n\n# Merge to 4bit\nmodel.push_to_hub_merged(\"rafatsiddiqui/Meta-Llama-3-8B-SST-FineTune-4bit\", tokenizer, save_method = \"merged_4bit\", token = hf_token)\n\n# Just LoRA adapters\nmodel.push_to_hub_merged(\"rafatsiddiqui/Meta-Llama-3-8B-SST-FineTune-LoRA\", tokenizer, save_method = \"lora\", token = hf_token)","metadata":{"_uuid":"1b4f163b-6507-46ce-ae34-941b2b6085e5","_cell_guid":"e85b21ce-0cad-4024-b60b-5cdb06ba5e29","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%ap2\n\nprint()","metadata":{"_uuid":"cac6aef5-87d3-4090-abce-2edfec7292f3","_cell_guid":"5d32136a-b8a4-459d-a997-d65ef5ea8d61","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"%%capture\n\n# From scratch\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n\n# Install necessary libraries without dependencies\n!pip install -U mlflow datasets transformers evaluate autoawq seaborn vllm xformers triton --no-cache-dir\n\n# Check CUDA version and NVIDIA driver status\n!nvcc --version\n!nvidia-smi\n\n# Install PyTorch and related packages\n!pip install -U torch torchvision torchaudio --no-cache-dir\n\n# Set environment variables for optimized performance\n!export OMP_NUM_THREADS=1\n!export MKL_NUM_THREADS=1\n\n# Verify CUDA availability in PyTorch\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n\n# ==========================================================================================================================================\n\n# # From Cache\n# %%capture\n\n# # Install Unsloth, Xformers, and other dependencies\n# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# # Install necessary libraries without dependencies\n# !pip install --no-deps mlflow datasets transformers peft accelerate trl evaluate autoawq seaborn triton\n\n# # Check CUDA version and NVIDIA driver status\n# !nvcc --version\n# !nvidia-smi\n\n# # Install PyTorch and related packages\n# !pip install torch torchvision torchaudio\n\n# !pip install triton\n\n# # Set environment variables for optimized performance\n# !export OMP_NUM_THREADS=1\n# !export MKL_NUM_THREADS=1\n\n# ==========================================================================================================================================\n\n\n# Verify CUDA availability in PyTorch\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\nimport os\n\nos.environ['hf_token'] = \"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\"\n\nfrom dotenv import load_dotenv\n\nload_dotenv()  # take environment variables from .env.\n\nhf_token = os.getenv('hf_token')\n\nprint(hf_token)\n\n# Import required libraries\nimport unsloth\nimport bitsandbytes\nimport mlflow\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, AutoTokenizer\nfrom unsloth import is_bfloat16_supported\nfrom trl import SFTTrainer\nimport evaluate\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport shutil\n\n# Clear cache directory\ncache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\nif os.path.exists(cache_dir):\n    shutil.rmtree(cache_dir)\nos.makedirs(cache_dir)\n\n# Check if CUDA is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize MLflow\nmlflow.set_tracking_uri(\"file:///mnt/data/mlruns\")\nmlflow.set_experiment(\"unsloth-train\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\ndtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = False  # Use 4bit quantization to reduce memory usage. Can be False.\n\n\nfrom unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"rafatsiddiqui/unsloth-llama-3-8b-bnb-4bit-stanford-sst3\", # YOUR MODEL YOU USED FOR TRAINING\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# alpaca_prompt = You MUST copy from above!\n\n\nsarcastic_statements = [\n    \"Oh great, another meeting. Just what I needed to make my day perfect.\",\n    \"Wow, you're so good at giving half the effort and still expecting full credit.\",\n    \"Yeah, because nothing says 'professional' like missing every single deadline.\",\n    \"Oh, you forgot again? Shocking.\",\n    \"Amazing, you managed to turn a simple task into a complete disaster. Bravo!\",\n    \"Oh, please, tell me more about how everyone else is wrong and you're always right.\",\n    \"Fantastic, another software update that makes everything worse.\",\n    \"Sure, let's keep doing it your way since it's worked out so brilliantly so far.\",\n    \"Oh, how convenient that you showed up right when the work is done.\",\n    \"Wow, your ability to avoid responsibility is truly inspirational.\"\n]\n\nalpaca_prompt = \"\"\"\nBelow is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the task.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\n\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\nfor i in sarcastic_statements:\n    inputs = tokenizer(\n    [\n        alpaca_prompt.format(\n            \"Identify the sentiment in the text below\", # instruction\n            i, # input\n            \"\", # output - leave this blank for generation!\n        )\n    ], return_tensors = \"pt\").to(\"cuda\")\n\n    from transformers import TextStreamer\n    text_streamer = TextStreamer(tokenizer)\n    _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sarcastic_statements = [\n    \"Oh great, another meeting. Just what I needed to make my day perfect.\",\n    \"Wow, you're so good at giving half the effort and still expecting full credit.\",\n    \"Yeah, because nothing says 'professional' like missing every single deadline.\",\n    \"Oh, you forgot again? Shocking.\",\n    \"Amazing, you managed to turn a simple task into a complete disaster. Bravo!\",\n    \"Oh, please, tell me more about how everyone else is wrong and you're always right.\",\n    \"Fantastic, another software update that makes everything worse.\",\n    \"Sure, let's keep doing it your way since it's worked out so brilliantly so far.\",\n    \"Oh, how convenient that you showed up right when the work is done.\",\n    \"Wow, your ability to avoid responsibility is truly inspirational.\"\n]\n\nalpaca_prompt = \"\"\"\nBelow is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the task.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\n\"\"\"\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\nfor i in sarcastic_statements:\n    inputs = tokenizer(\n    [\n        alpaca_prompt.format(\n            \"Identify the sentiment in the text below\", # instruction\n            i, # input\n            \"\", # output - leave this blank for generation!\n        )\n    ], return_tensors = \"pt\").to(\"cuda\")\n\n    from transformers import TextStreamer\n    text_streamer = TextStreamer(tokenizer)\n    _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finito","metadata":{}},{"cell_type":"markdown","source":"### Using accelerate to see if any gains in training time","metadata":{"_uuid":"490b13b9-af6b-4d30-98a9-891253138376","_cell_guid":"56adc15a-45e0-4632-9715-aae527294247","trusted":true}},{"cell_type":"code","source":"from accelerate import Accelerator\naccelerator = Accelerator()\nmodel, train_dataset, eval_dataset = accelerator.prepare(model, train_dataset, test_dataset)","metadata":{"_uuid":"2b26d4fd-a06b-47a9-bf8e-de736566c4ed","_cell_guid":"0f68669c-55ab-4a5d-9fad-9170b16b3433","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import accelerate\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport torch\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Initialize the accelerator\naccelerator = accelerate.Accelerator()\n\n# Function to get predictions from vllm using batch processing\ndef get_vllm_predictions(model, tokenizer, dataset, batch_size=32):\n    model.eval()\n    predictions, labels = [], []\n    for i in range(0, len(dataset), batch_size):\n        batch = dataset[i:i+batch_size]\n        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", max_length=max_seq_length, truncation=True, padding=True).to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n        predictions.extend(preds)\n        labels.extend(batch[\"label\"])\n    return predictions, labels\n\n# Load tokenizer and model (assuming they are defined earlier in the script)\n# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n# model = SomeModelClass.from_pretrained(\"model_name\")\n\n# Load validation and test sets and group into 3 labels\nval_set = load_dataset(\"SetFit/sst5\", split=\"validation\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\ntest_set = load_dataset(\"SetFit/sst5\", split=\"test\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\n\n# Ensure labels are integers\nval_set = val_set.map(lambda example: {\"label\": int(example[\"label\"])})\ntest_set = test_set.map(lambda example: {\"label\": int(example[\"label\"])})\n\n# Move model to the correct device\nmodel = accelerator.prepare(model)\n\n# Get predictions\nval_predictions, val_labels = get_vllm_predictions(model, tokenizer, val_set)\ntest_predictions, test_labels = get_vllm_predictions(model, tokenizer, test_set)\n\n# Ensure predictions and labels are numpy arrays of integer type\nval_predictions = np.array(val_predictions, dtype=int)\nval_labels = np.array(val_labels, dtype=int)\ntest_predictions = np.array(test_predictions, dtype=int)\ntest_labels = np.array(test_labels, dtype=int)\n\n# Check for consistency in the data types and lengths\nprint(\"Validation Labels Type:\", val_labels.dtype, \"Length:\", len(val_labels))\nprint(\"Validation Predictions Type:\", val_predictions.dtype, \"Length:\", len(val_predictions))\nprint(\"Test Labels Type:\", test_labels.dtype, \"Length:\", len(test_labels))\nprint(\"Test Predictions Type:\", test_predictions.dtype, \"Length:\", len(test_predictions))\n\n# Calculate confusion matrix and classification report\nval_conf_matrix = confusion_matrix(val_labels, val_predictions)\ntest_conf_matrix = confusion_matrix(test_labels, test_predictions)\n\n# Plot confusion matrix for validation set\nplt.figure(figsize=(10, 7))\nsns.heatmap(val_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Validation Set Confusion Matrix')\nplt.show()\n\n# Plot confusion matrix for test set\nplt.figure(figsize=(10, 7))\nsns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Test Set Confusion Matrix')\nplt.show()\n\n# Print classification reports\nprint(\"Validation Set Classification Report:\")\nprint(classification_report(val_labels, val_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n\nprint(\"Test Set Classification Report:\")\nprint(classification_report(test_labels, test_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))","metadata":{"_uuid":"a0fdf299-ff11-4540-a74f-cf09a7c5873f","_cell_guid":"9e2ddb63-122a-4849-84d3-ca90afc9f0a3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# End","metadata":{"_uuid":"c8107de1-1dc7-4329-a86c-a549937eac0d","_cell_guid":"bbacade7-4821-49de-b01c-3fbecf4110bc","trusted":true}},{"cell_type":"code","source":"from accelerate import Accelerator\naccelerator = Accelerator()\nmodel, train_dataset, eval_dataset = accelerator.prepare(model, train_dataset, test_dataset)","metadata":{"_uuid":"72363bdc-3d6f-44b1-88c7-a6d03f031d79","_cell_guid":"da6d1b1a-9e98-4e23-bdcc-90c19a3e1ecf","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=2,\n    learning_rate=2e-4,\n    fp16=torch.cuda.is_available() and not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=sst5,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False, # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\ntrainer.train()\n\n# Log parameters and metrics\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\nmlflow.log_metric(\"final_loss\", trainer.state.log_history[-1]['train_loss'])","metadata":{"_uuid":"2a2331f4-f460-416e-b354-f3313876cc4e","_cell_guid":"9c27cc66-fbc1-48d3-9a9b-37aef50a437c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, AutoTokenizer\nfrom trl import SFTTrainer\nimport mlflow\nimport torch\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom accelerate import Accelerator\n\n# Initialize the Accelerator\naccelerator = Accelerator()\n\n# Model configuration\nmax_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\ndtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = False  # Use 4bit quantization to reduce memory usage. Can be False.\n\n# Load model and tokenizer\nmodel_name = \"meta-llama/Meta-Llama-3-8B\"\nhf_token = \"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\"  # Ensure you have your Hugging Face token\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n    token=hf_token,  # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=2,\n    learning_rate=2e-4,\n    fp16=torch.cuda.is_available() and not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Use Accelerator for device placement\nmodel, train_dataset = accelerator.prepare(model, train_dataset)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,  # Ensure this is correctly defined\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,  # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\n# Start MLflow logging\nmlflow.start_run()\n\n# Log parameters\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\n# Train the model\ntrainer.train()\n\n# Log final training loss\nfinal_loss = trainer.state.log_history[-1]['loss']  # Ensure correct key\nmlflow.log_metric(\"final_loss\", final_loss)\n\n# End MLflow run\nmlflow.end_run()","metadata":{"_uuid":"05a0ce1e-f3c7-4e38-afd5-f259f66865db","_cell_guid":"8f64bd9e-45f6-4667-99a7-21893dd41bac","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nimport os\nimport shutil\n\n# Hugging Face cache directory\nhf_cache_dir = os.path.expanduser(\"~/.cache/huggingface\")\n\n# Clear Hugging Face cache\nif os.path.exists(hf_cache_dir):\n    shutil.rmtree(hf_cache_dir)\n    print(f\"Hugging Face cache cleared at {hf_cache_dir}\")\nelse:\n    print(f\"No Hugging Face cache found at {hf_cache_dir}\")\n\n# PyTorch cache directory\ntorch_cache_dir = os.path.expanduser(\"~/.cache/torch\")\n\n# Clear PyTorch cache\nif os.path.exists(torch_cache_dir):\n    shutil.rmtree(torch_cache_dir)\n    print(f\"PyTorch cache cleared at {torch_cache_dir}\")\nelse:\n    print(f\"No PyTorch cache found at {torch_cache_dir}\")\n\n# Additional step to clear CUDA cache if necessary\ntorch.cuda.empty_cache()\nprint(\"CUDA cache cleared\")","metadata":{"_uuid":"ba4a5849-2b62-446d-85b0-c466851b3d60","_cell_guid":"9a1a0cdb-b447-45c9-a5ce-06135f9ff928","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer, AutoConfig\n\ndef check_model_files(model_path):\n    required_files = [\"config.json\", \"pytorch_model.bin\", \"tokenizer_config.json\"]\n    missing_files = [file for file in required_files if not os.path.isfile(os.path.join(model_path, file))]\n    if missing_files:\n        raise FileNotFoundError(f\"The following required files are missing from {model_path}: {', '.join(missing_files)}\")\n    else:\n        print(\"All required model files are present.\")\n\n# Define the model path\nmodel_path = \"/kaggle/working\"\n\n# Check for required files\ncheck_model_files(model_path)\n\n# Quantization configuration\nquant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4}\n\n# Load model\ntry:\n    model = AutoAWQForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    print(\"Model and tokenizer loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading the model: {e}\")","metadata":{"_uuid":"93ed84ca-d980-4152-bfa0-b69209145bad","_cell_guid":"eedfee29-b9d3-4051-9fce-78e340dc8698","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import accelerate\n# from datasets import load_dataset\n# from transformers import AutoTokenizer\n# import torch\n# import numpy as np\n# from sklearn.metrics import confusion_matrix, classification_report\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# # Initialize the accelerator\n# accelerator = accelerate.Accelerator()\n\n# # Function to get predictions from vllm using batch processing\n# def get_vllm_predictions(model, tokenizer, dataset, batch_size=32):\n#     model.eval()\n#     predictions, labels = [], []\n#     for i in range(0, len(dataset), batch_size):\n#         batch = dataset[i:i+batch_size]\n#         inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", max_length=max_seq_length, truncation=True, padding=True).to(device)\n#         with torch.no_grad():\n#             outputs = model(**inputs)\n#         logits = outputs.logits\n#         preds = torch.argmax(logits, dim=-1).cpu().numpy()\n#         predictions.extend(preds)\n#         labels.extend(batch[\"label\"])\n#     return predictions, labels\n\n# # Load tokenizer and model (assuming they are defined earlier in the script)\n# # tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n# # model = SomeModelClass.from_pretrained(\"model_name\")\n\n# # Load validation and test sets and group into 3 labels\n# val_set = load_dataset(\"SetFit/sst5\", split=\"validation\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\n# test_set = load_dataset(\"SetFit/sst5\", split=\"test\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\n\n# # Ensure labels are integers\n# val_set = val_set.map(lambda example: {\"label\": int(example[\"label\"])})\n# test_set = test_set.map(lambda example: {\"label\": int(example[\"label\"])})\n\n# # Move model to the correct device\n# model = accelerator.prepare(model)\n\n# # Get predictions\n# val_predictions, val_labels = get_vllm_predictions(model, tokenizer, val_set)\n# test_predictions, test_labels = get_vllm_predictions(model, tokenizer, test_set)\n\n# # Ensure predictions and labels are numpy arrays of integer type\n# val_predictions = np.array(val_predictions, dtype=int)\n# val_labels = np.array(val_labels, dtype=int)\n# test_predictions = np.array(test_predictions, dtype=int)\n# test_labels = np.array(test_labels, dtype=int)\n\n# # Check for consistency in the data types and lengths\n# print(\"Validation Labels Type:\", val_labels.dtype, \"Length:\", len(val_labels))\n# print(\"Validation Predictions Type:\", val_predictions.dtype, \"Length:\", len(val_predictions))\n# print(\"Test Labels Type:\", test_labels.dtype, \"Length:\", len(test_labels))\n# print(\"Test Predictions Type:\", test_predictions.dtype, \"Length:\", len(test_predictions))\n\n# # Calculate confusion matrix and classification report\n# val_conf_matrix = confusion_matrix(val_labels, val_predictions)\n# test_conf_matrix = confusion_matrix(test_labels, test_predictions)\n\n# # Plot confusion matrix for validation set\n# plt.figure(figsize=(10, 7))\n# sns.heatmap(val_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n# plt.xlabel('Predicted')\n# plt.ylabel('Actual')\n# plt.title('Validation Set Confusion Matrix')\n# plt.show()\n\n# # Plot confusion matrix for test set\n# plt.figure(figsize=(10, 7))\n# sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n# plt.xlabel('Predicted')\n# plt.ylabel('Actual')\n# plt.title('Test Set Confusion Matrix')\n# plt.show()\n\n# # Print classification reports\n# print(\"Validation Set Classification Report:\")\n# print(classification_report(val_labels, val_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n\n# print(\"Test Set Classification Report:\")\n# print(classification_report(test_labels, test_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))","metadata":{"_uuid":"a3bc2ae9-0c92-4ff4-95d8-169f35b9dd7c","_cell_guid":"a6c79d0c-8865-4d59-91c4-aa6ac977fe6e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}