{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set Up","metadata":{"_uuid":"6dde6acf-6d75-41c2-a2a6-571518c66ba3","_cell_guid":"cdf3ee35-a08a-43ad-baeb-66f7e4cbb7ad","trusted":true}},{"cell_type":"code","source":"%reset","metadata":{"_uuid":"a8632913-1586-488c-bf86-bb1153027224","_cell_guid":"4fd5ba6e-9592-4717-b5dc-562968ed6617","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir('/')","metadata":{"_uuid":"61f460b8-54d3-4a7e-9d3b-8e517945ab02","_cell_guid":"949bb46b-ff61-4574-9561-eb64d83dbc8c","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:00:06.645631Z","iopub.execute_input":"2024-06-23T18:00:06.645933Z","iopub.status.idle":"2024-06-23T18:00:06.651176Z","shell.execute_reply.started":"2024-06-23T18:00:06.645894Z","shell.execute_reply":"2024-06-23T18:00:06.650021Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git status","metadata":{"execution":{"iopub.status.busy":"2024-06-24T00:56:48.778561Z","iopub.execute_input":"2024-06-24T00:56:48.779201Z","iopub.status.idle":"2024-06-24T00:56:49.718126Z","shell.execute_reply.started":"2024-06-24T00:56:48.779167Z","shell.execute_reply":"2024-06-24T00:56:49.717226Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"fatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls -a","metadata":{"_uuid":"ad12e78a-f5ea-49ff-8560-d5b8b90881b8","_cell_guid":"eaff0ecd-f3d3-4466-aec7-eabc85792aac","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:00:06.652369Z","iopub.execute_input":"2024-06-23T18:00:06.652644Z","iopub.status.idle":"2024-06-23T18:00:07.606394Z","shell.execute_reply.started":"2024-06-23T18:00:06.652621Z","shell.execute_reply":"2024-06-23T18:00:07.605506Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nfrom prettytable import PrettyTable\n\ndef find_notebooks(start_path='.'):\n    notebooks = []\n    for root, dirs, files in os.walk(start_path):\n        for file in files:\n            if file.endswith('.ipynb'):\n                full_path = os.path.join(root, file)\n                mod_time = os.path.getmtime(full_path)\n                mod_date = datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S')\n                notebooks.append((file, full_path, mod_date))\n    return notebooks\n\ndef create_notebook_table(notebooks):\n    table = PrettyTable()\n    table.field_names = [\"Name\", \"Location\", \"Last Modified\"]\n    table.align[\"Name\"] = \"l\"\n    table.align[\"Location\"] = \"l\"\n    table.align[\"Last Modified\"] = \"l\"\n    for notebook in notebooks:\n        table.add_row(notebook)\n    return table\n\n# Find all notebooks\nnotebooks = find_notebooks('/')\n\n# Create and print the table\ntable = create_notebook_table(notebooks)\nprint(table)\n\n# Print total count\nprint(f\"\\nTotal notebooks found: {len(notebooks)}\")","metadata":{"_uuid":"357393e6-a20f-4dbe-a61b-c98dfcde6396","_cell_guid":"3407f15c-01b3-4782-8dad-4cfae6057cbe","collapsed":false,"scrolled":true,"execution":{"iopub.status.busy":"2024-06-23T18:00:07.607761Z","iopub.execute_input":"2024-06-23T18:00:07.608069Z","iopub.status.idle":"2024-06-23T18:00:11.645909Z","shell.execute_reply.started":"2024-06-23T18:00:07.608043Z","shell.execute_reply":"2024-06-23T18:00:11.644970Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python --version","metadata":{"_uuid":"7e89b17e-6958-41b7-bd94-ddc300345426","_cell_guid":"67a88632-a1d0-48e9-82d9-bc47a73d03a5","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:00:56.592825Z","iopub.execute_input":"2024-06-23T18:00:56.593214Z","iopub.status.idle":"2024-06-23T18:00:57.536406Z","shell.execute_reply.started":"2024-06-23T18:00:56.593182Z","shell.execute_reply":"2024-06-23T18:00:57.535497Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git config --global user.email \"mdrafat.siddiqui@outlook.com\"","metadata":{"_uuid":"895ede56-32aa-4997-83f9-bed60972ceab","_cell_guid":"170e8502-53ae-4b33-a88f-35ec31b16923","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:01:00.811474Z","iopub.execute_input":"2024-06-23T18:01:00.811841Z","iopub.status.idle":"2024-06-23T18:01:01.817798Z","shell.execute_reply.started":"2024-06-23T18:01:00.811811Z","shell.execute_reply":"2024-06-23T18:01:01.816375Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"/opt/conda/pkgs/conda-4.12.0-py310h06a4308_0/info/test/tests/conda_env/support/notebook.ipynb\"                                 \"\"","metadata":{"_uuid":"4d4e03b0-ea0c-4c23-b01c-83edf4715838","_cell_guid":"f9ee7dcd-f132-4b65-aac8-f15c5acb8158","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:01:29.561311Z","iopub.execute_input":"2024-06-23T18:01:29.561623Z","iopub.status.idle":"2024-06-23T18:01:29.570071Z","shell.execute_reply.started":"2024-06-23T18:01:29.561600Z","shell.execute_reply":"2024-06-23T18:01:29.568241Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd '/opt/conda/pkgs/conda-4.12.0-py310h06a4308_0/info/test/tests/conda_env/support/'","metadata":{"_uuid":"1e88212a-1df8-4d5d-b879-93f25fe387b0","_cell_guid":"2632cd2e-1f3e-4f01-bc20-faaa1a44000b","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:02:11.240765Z","iopub.execute_input":"2024-06-23T18:02:11.241116Z","iopub.status.idle":"2024-06-23T18:02:11.248377Z","shell.execute_reply.started":"2024-06-23T18:02:11.241090Z","shell.execute_reply":"2024-06-23T18:02:11.247379Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -a","metadata":{"_uuid":"91534365-4761-46a1-9509-cb3368199933","_cell_guid":"5f78e228-31c6-4f5f-a5fa-940b7cade979","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:02:19.247749Z","iopub.execute_input":"2024-06-23T18:02:19.248560Z","iopub.status.idle":"2024-06-23T18:02:20.230947Z","shell.execute_reply.started":"2024-06-23T18:02:19.248529Z","shell.execute_reply":"2024-06-23T18:02:20.229983Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Magic Functions for Github","metadata":{"_uuid":"2122a74f-8729-40b6-af9a-cef2810e8508","_cell_guid":"8b375b53-3c74-4ff6-8edc-b661e7db2403","trusted":true}},{"cell_type":"code","source":"from IPython.core.magic import register_cell_magic\nimport subprocess\nimport re\nfrom datetime import datetime\nimport pytz\nimport threading\nimport time\n\ndef delete_branch(branch_name):\n    try:\n        # Delete the branch locally\n        subprocess.run([\"git\", \"branch\", \"-D\", branch_name], check=True)\n        # Delete the branch remotely\n        subprocess.run([\"git\", \"push\", \"origin\", \"--delete\", branch_name], check=True)\n        print(f\"Deleted branch '{branch_name}'\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error deleting branch: {e}\")\n\ndef save_and_push(is_auto=False):\n    # Get current Indian Standard Time\n    ist = datetime.now(pytz.timezone('Asia/Kolkata'))\n    branch_name = ist.strftime(\"%d-%b-%Y-%H%M-IST\")\n    if is_auto:\n        branch_name += \"-autosave\"\n    \n    try:\n        # If it's an auto-save, delete the previous auto-save branch\n        if is_auto:\n            subprocess.run([\"git\", \"fetch\", \"--all\"], check=True)\n            result = subprocess.run([\"git\", \"branch\", \"-r\"], capture_output=True, text=True, check=True)\n            for branch in result.stdout.split('\\n'):\n                if \"-autosave\" in branch:\n                    delete_branch(branch.strip().split('/')[-1])\n        \n        subprocess.run([\"git\", \"checkout\", \"-b\", branch_name], check=True)\n        subprocess.run([\"git\", \"add\", \".\"], check=True)\n        commit_message = f\"Auto-save at {branch_name}\" if is_auto else f\"Manual save at {branch_name}\"\n        subprocess.run([\"git\", \"commit\", \"-m\", commit_message], check=True)\n        subprocess.run([\"git\", \"push\", \"origin\", branch_name], check=True)\n        print(f\"Changes pushed to new branch '{branch_name}' successfully.\")\n    except subprocess.CalledProcessError as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        subprocess.run([\"git\", \"checkout\", \"main\"], check=True)\n\ndef auto_save_thread():\n    while True:\n        time.sleep(120)  # Wait for 2 minutes\n        save_and_push(is_auto=True)\n\n# Start the auto-save thread\nthreading.Thread(target=auto_save_thread, daemon=True).start()\n\n@register_cell_magic\ndef ap1(line, cell):\n    exec(cell, globals())\n    save_and_push()\n\n@register_cell_magic\ndef ap2(line, cell):\n    exec(cell, globals())\n    save_and_push()\n\nip = get_ipython()\nip.register_magic_function(ap1, 'cell')\nip.register_magic_function(ap2, 'cell')\n\nprint(\"Auto-save and push system set up. It will run every 2 minutes.\")","metadata":{"_uuid":"21c533c5-5515-446f-87da-63dc77b81ef2","_cell_guid":"7376188c-4c72-494c-b5b4-17dad94b2b17","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:02:44.070314Z","iopub.execute_input":"2024-06-23T18:02:44.070687Z","iopub.status.idle":"2024-06-23T18:02:44.090732Z","shell.execute_reply.started":"2024-06-23T18:02:44.070657Z","shell.execute_reply":"2024-06-23T18:02:44.089762Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf .git\n!git init\n!git add .\n!git commit -m \"commit\"\n!git branch -M main\n!git remote add origin https://rafatsiddiqui9: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@github.com/rafatsiddiqui9/e2e-llm-backup.git\n!git push -u origin main\n!git fetch origin main\n!git pull --rebase origin main\n!git push -u origin main","metadata":{"_uuid":"19a46395-a9e3-4e3c-a627-1b4dd0bfb695","_cell_guid":"34984760-83f5-463e-be56-8452d161f37c","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:03:29.815618Z","iopub.execute_input":"2024-06-23T18:03:29.816233Z","iopub.status.idle":"2024-06-23T18:03:36.898306Z","shell.execute_reply.started":"2024-06-23T18:03:29.816195Z","shell.execute_reply":"2024-06-23T18:03:36.897347Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git fetch origin main\n!git pull --rebase origin main\n!git push -u origin main","metadata":{"_uuid":"facab489-759f-40f0-be65-490a897c3313","_cell_guid":"7f2c6c10-2be5-4bb8-9163-46ea2d16ac18","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:03:51.284567Z","iopub.execute_input":"2024-06-23T18:03:51.284958Z","iopub.status.idle":"2024-06-23T18:03:55.107985Z","shell.execute_reply.started":"2024-06-23T18:03:51.284927Z","shell.execute_reply":"2024-06-23T18:03:55.107030Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%ap2\n\nprint()","metadata":{"_uuid":"41948245-015f-4516-b0f7-24cdfa36559f","_cell_guid":"f10daa8b-812d-4c2d-9d39-1ecf7e9485ef","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:04:05.085903Z","iopub.execute_input":"2024-06-23T18:04:05.086280Z","iopub.status.idle":"2024-06-23T18:04:05.110910Z","shell.execute_reply.started":"2024-06-23T18:04:05.086253Z","shell.execute_reply":"2024-06-23T18:04:05.110059Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Magic Function for Visual Feedback","metadata":{"_uuid":"2373f2a4-e21e-49ea-bd56-8d396289174f","_cell_guid":"66850348-d8cc-4044-b134-2863b32c282b","trusted":true}},{"cell_type":"code","source":"from IPython.display import display, HTML\nfrom IPython import get_ipython\n\ndef display_feedback(success):\n    color = \"green\" if success else \"red\"\n    display(HTML(f\"<div style='width:20px;height:20px;background-color:{color};border-radius:50%;'></div>\"))\n\ndef visual_feedback_after_execution(result):\n    success = not (result.error_before_exec or result.error_in_exec)\n    display_feedback(success)\n\n# Get the current IPython instance\nip = get_ipython()\n\n# Remove the audio hook\nfor hook in ip.events.callbacks['post_run_cell']:\n    if 'play_audio_after_execution' in str(hook):\n        ip.events.callbacks['post_run_cell'].remove(hook)\n\n# Register the visual feedback hook\nip.events.register('post_run_cell', visual_feedback_after_execution)\n\nprint(\"Visual feedback system set up.\")","metadata":{"_uuid":"d711fe19-2fcc-438e-bb93-898b3e018eb8","_cell_guid":"2e6617f5-37a8-4bb4-97d8-7c05938cce87","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:04:10.476419Z","iopub.execute_input":"2024-06-23T18:04:10.476764Z","iopub.status.idle":"2024-06-23T18:04:10.486599Z","shell.execute_reply.started":"2024-06-23T18:04:10.476737Z","shell.execute_reply":"2024-06-23T18:04:10.485680Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Magic Function for sound","metadata":{"_uuid":"f7788163-3183-44da-b165-ce620ba69a29","_cell_guid":"0120c5d3-07a9-4ccd-953a-ed1036e1ad81","trusted":true}},{"cell_type":"code","source":"# !pip install pydub --no-cache-dir\n\nimport requests\n\n# Google Drive file ID\nfile_id = \"12v2RwutER9ayuPe4d4H0GQAxA9iUgdXx\"\nurl = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Save the content of the response as an MP3 file\nwith open(\"error.mp3\", \"wb\") as file:\n    file.write(response.content)\n\nprint(\"File downloaded and saved as error.mp3\")\n\nimport requests\n\n# Google Drive file ID\nfile_id = \"1-kfEx5SgDxB0ph71q9BhKAEB8R4qShFx\"\nurl = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Save the content of the response as an MP3 file\nwith open(\"success.mp3\", \"wb\") as file:\n    file.write(response.content)\n\nprint(\"File downloaded and saved as success.mp3\")\n\nfrom IPython.display import display, Audio\nfrom IPython import get_ipython\nfrom pydub import AudioSegment\n\n# Define the post-execution hook\ndef play_audio_after_execution(result):\n    if result.error_before_exec or result.error_in_exec:\n        # Load and adjust volume of the error sound\n        audio = AudioSegment.from_mp3(\"error.mp3\")\n    else:\n        # Load and adjust volume of the success sound\n        audio = AudioSegment.from_mp3(\"success.mp3\")\n\n    # Set volume to 50%\n    audio = audio - 25 # Reduce volume by 10dB (approximately 50%)\n\n    # Export to a temporary file and play\n    audio.export(\"temp_output.mp3\", format=\"mp3\")\n    display(Audio(\"temp_output.mp3\", autoplay=True))\n\n# Get the current IPython instance\nip = get_ipython()\n\n# Register the post-execution hook\nip.events.register('post_run_cell', play_audio_after_execution)","metadata":{"_uuid":"05e45631-3195-4735-aa05-7292683459f4","_cell_guid":"958ce95f-8e5a-49b6-9764-4e0ad83ba8b8","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:04:12.196708Z","iopub.execute_input":"2024-06-23T18:04:12.197445Z","iopub.status.idle":"2024-06-23T18:04:19.611186Z","shell.execute_reply.started":"2024-06-23T18:04:12.197401Z","shell.execute_reply":"2024-06-23T18:04:19.610135Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Magic Function for Updating Conda Libraries","metadata":{"_uuid":"3189bb7d-7edf-40c0-aa66-15ccbfb9277b","_cell_guid":"ba47ce07-600f-405f-8995-dcba93657bda","trusted":true}},{"cell_type":"code","source":"# import subprocess\n# import time\n# from tqdm import tqdm\n\n# print(\"Updating all packages in the current Conda environment...\")\n\n# # Start the conda update process\n# process = subprocess.Popen([\"conda\", \"update\", \"--all\", \"-y\"], \n#                            stdout=subprocess.PIPE, \n#                            stderr=subprocess.STDOUT,\n#                            universal_newlines=True)\n\n# # Create a progress bar\n# with tqdm(total=100, desc=\"Updating\", bar_format=\"{l_bar}{bar}\", ncols=50) as pbar:\n#     # Read the output line by line\n#     for line in iter(process.stdout.readline, ''):\n#         if line:\n#             # Update progress bar\n#             pbar.update(1)\n#             time.sleep(0.1)  # Add a small delay to make the progress visible\n        \n#         # Check if process has finished\n#         if process.poll() is not None:\n#             break\n\n# # Ensure the progress bar reaches 100%\n# pbar.update(100 - pbar.n)\n\n# # Check the return code\n# if process.returncode != 0:\n#     print(f\"Error occurred. Return code: {process.returncode}\")\n# else:\n#     print(\"Update complete!\")","metadata":{"_uuid":"10f03123-912f-4025-be10-766652d10521","_cell_guid":"309b2448-dde7-4c95-a7e6-51564babe5ac","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:04:19.612681Z","iopub.execute_input":"2024-06-23T18:04:19.613236Z","iopub.status.idle":"2024-06-23T18:04:19.884261Z","shell.execute_reply.started":"2024-06-23T18:04:19.613205Z","shell.execute_reply":"2024-06-23T18:04:19.883318Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git config --global user.email \"mdrafat.siddiqui@outlook.com\"","metadata":{"_uuid":"90d20f3e-7aab-4386-8c76-24f0768a5aa3","_cell_guid":"f897f594-569f-42f6-8769-2a96f3f6e9bb","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:04:22.774326Z","iopub.execute_input":"2024-06-23T18:04:22.774777Z","iopub.status.idle":"2024-06-23T18:04:23.966579Z","shell.execute_reply.started":"2024-06-23T18:04:22.774722Z","shell.execute_reply":"2024-06-23T18:04:23.965469Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main Workflow","metadata":{"_uuid":"8ecef8c7-099a-4bd0-801b-9419dd4ac2a2","_cell_guid":"2644d883-3a81-4dd0-a291-e1bec6f500b6","trusted":true}},{"cell_type":"markdown","source":"## Install Libraries from SCRATCH","metadata":{"_uuid":"83895670-704c-4385-b696-6ba967539e66","_cell_guid":"34ccf1f5-6d91-423b-89fd-5006705aceb1","trusted":true}},{"cell_type":"code","source":"%%capture\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes","metadata":{"_uuid":"d941584f-4d1f-47ce-8e77-5ea067f0440e","_cell_guid":"191b58b1-8ae9-4751-ae8a-44b77a32ff2c","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:04:48.083254Z","iopub.execute_input":"2024-06-23T18:04:48.083612Z","iopub.status.idle":"2024-06-23T18:05:35.992399Z","shell.execute_reply.started":"2024-06-23T18:04:48.083582Z","shell.execute_reply":"2024-06-23T18:05:35.991296Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n\n# Install necessary libraries without dependencies\n!pip install -U mlflow datasets transformers evaluate autoawq seaborn vllm xformers triton --no-cache-dir\n\n# Check CUDA version and NVIDIA driver status\n!nvcc --version\n!nvidia-smi\n\n# Install PyTorch and related packages\n!pip install -U torch torchvision torchaudio --no-cache-dir\n\n# Set environment variables for optimized performance\n!export OMP_NUM_THREADS=1\n!export MKL_NUM_THREADS=1\n\n# Verify CUDA availability in PyTorch\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"_uuid":"d3286683-480f-48ef-8a5d-a366a59d6fd9","_cell_guid":"89a653e4-abd5-42a2-8d92-dfc44d53e954","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:05:35.994628Z","iopub.execute_input":"2024-06-23T18:05:35.994959Z","iopub.status.idle":"2024-06-23T18:09:36.823934Z","shell.execute_reply.started":"2024-06-23T18:05:35.994930Z","shell.execute_reply":"2024-06-23T18:09:36.822938Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Install libraries from CACHE","metadata":{"_uuid":"6853cbf7-4706-49fb-b18f-f91b6f404cb4","_cell_guid":"9c8ee625-8cda-4de6-844e-28cd163921f8","trusted":true}},{"cell_type":"code","source":"# %%capture\n\n# # Install Unsloth, Xformers, and other dependencies\n# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n# \n# # Install necessary libraries without dependencies\n# !pip install --no-deps mlflow datasets transformers peft accelerate trl evaluate autoawq seaborn triton\n\n# # Check CUDA version and NVIDIA driver status\n# !nvcc --version\n# !nvidia-smi\n\n# # Install PyTorch and related packages\n# !pip install torch torchvision torchaudio\n\n# !pip install triton\n\n# # Set environment variables for optimized performance\n# !export OMP_NUM_THREADS=1\n# !export MKL_NUM_THREADS=1\n\n# # Verify CUDA availability in PyTorch\n# import torch\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# print(device)","metadata":{"_uuid":"86b9ed8d-2cb0-4ad7-96a0-2150153a9281","_cell_guid":"b90a4ef4-b983-4da1-93ff-afa3ccbb92f5","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T17:58:28.939371Z","iopub.execute_input":"2024-06-23T17:58:28.940243Z","iopub.status.idle":"2024-06-23T17:58:58.380152Z","shell.execute_reply.started":"2024-06-23T17:58:28.940208Z","shell.execute_reply":"2024-06-23T17:58:58.379008Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ['hf_token'] = \"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\"\n\nfrom dotenv import load_dotenv\n\nload_dotenv()  # take environment variables from .env.\n\nhf_token = os.getenv('hf_token')\n\nhf_token","metadata":{"_uuid":"d3d94e5a-f55b-41da-a853-6fd769f67166","_cell_guid":"bcf13b71-0588-42b0-8882-1c74ed567086","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:09:36.825293Z","iopub.execute_input":"2024-06-23T18:09:36.825737Z","iopub.status.idle":"2024-06-23T18:09:37.093275Z","shell.execute_reply.started":"2024-06-23T18:09:36.825708Z","shell.execute_reply":"2024-06-23T18:09:37.092284Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Libraries","metadata":{"_uuid":"ab8c15c0-9798-4118-a76d-7640179e9a18","_cell_guid":"2fff066b-2a1a-4883-9acd-1fa3c8a8cdc0","trusted":true}},{"cell_type":"code","source":"# Import required libraries\nimport unsloth\nimport bitsandbytes\nimport mlflow\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, AutoTokenizer\nfrom unsloth import is_bfloat16_supported\nfrom trl import SFTTrainer\nimport evaluate\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport shutil","metadata":{"_uuid":"48e8a36c-b492-42ed-afc8-d569e329e1f9","_cell_guid":"19fd8865-afc1-451f-b4ca-6cdddd8b7b5b","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:09:37.095309Z","iopub.execute_input":"2024-06-23T18:09:37.095590Z","iopub.status.idle":"2024-06-23T18:09:54.506307Z","shell.execute_reply.started":"2024-06-23T18:09:37.095567Z","shell.execute_reply":"2024-06-23T18:09:54.505065Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !export HUGGINGFACE_HUB_TOKEN= hf_token\n# !export TOKENIZERS_PARALLELISM=false","metadata":{"_uuid":"69385243-0c96-4d6a-8703-5534e0b143ee","_cell_guid":"a7652c60-b59a-4fa8-998a-6bc720214378","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:09:54.508035Z","iopub.execute_input":"2024-06-23T18:09:54.509031Z","iopub.status.idle":"2024-06-23T18:09:54.779332Z","shell.execute_reply.started":"2024-06-23T18:09:54.508978Z","shell.execute_reply":"2024-06-23T18:09:54.778312Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set up MLFlow","metadata":{"_uuid":"771d18d4-39e6-4e5e-a3a9-67da6a3d96fc","_cell_guid":"28854f2e-a905-464f-9b2c-b19d1b2f2c33","trusted":true}},{"cell_type":"code","source":"# Clear cache directory\ncache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\nif os.path.exists(cache_dir):\n    shutil.rmtree(cache_dir)\nos.makedirs(cache_dir)\n\n# Check if CUDA is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize MLflow\nmlflow.set_tracking_uri(\"file:///mnt/data/mlruns\")\nmlflow.set_experiment(\"unsloth-train\")","metadata":{"_uuid":"7bc87a22-4eda-48db-9fbe-5e4a3b590e13","_cell_guid":"0cbd9342-ba23-4629-bae3-20d929e20d61","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:09:54.780794Z","iopub.execute_input":"2024-06-23T18:09:54.781196Z","iopub.status.idle":"2024-06-23T18:09:55.138683Z","shell.execute_reply.started":"2024-06-23T18:09:54.781150Z","shell.execute_reply":"2024-06-23T18:09:55.137439Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vanilla Unsloth Code","metadata":{"_uuid":"38737e42-f8e1-4118-97c2-3958904c492c","_cell_guid":"3a817861-b01e-4d79-9e64-778c6bf5a16a","trusted":true}},{"cell_type":"markdown","source":"### Import unsloth model","metadata":{"_uuid":"ddfc4594-aa8c-4dc0-8062-35486857bc26","_cell_guid":"f422b3b3-ac23-4ebe-be8d-249f53071c4c","trusted":true}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"_uuid":"e62828b9-c378-4759-9c20-591d089b2857","_cell_guid":"cf924597-7b47-411f-8f5e-8230022d2080","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configure with PEFT","metadata":{"_uuid":"da2fc1b9-6efe-4971-a50e-727da429c0b9","_cell_guid":"05f6ee32-a725-447c-bb23-66bc7bc98856","trusted":true}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"_uuid":"a5353351-4d51-4910-9cd5-ab5393a9587b","_cell_guid":"c4950d0b-706b-466f-abb9-64a1b6a0d958","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Frame the prompt","metadata":{"_uuid":"e69aa6e6-b368-45ed-a537-a3528b930d85","_cell_guid":"22d42177-431d-4f4f-9df5-cc2b82334c15","trusted":true}},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add aEOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"_uuid":"79a8b917-4333-404d-9c73-147faf46636c","_cell_guid":"0a1e4394-6d41-47fc-95bd-d0e16fa9b9b2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Instantiate Supervised FineTuning Trainer and train","metadata":{"_uuid":"9ea8817a-25e9-431b-b457-36beb4be634f","_cell_guid":"d1ba3af5-a203-466e-9df8-f9aaf72cf01b","trusted":true}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        num_train_epochs = 5,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 5,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"_uuid":"2fb43587-b2a1-4062-b569-df2bd470bf12","_cell_guid":"de6d55ee-64e4-4cda-8a64-6b8ebe767334","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"_uuid":"50d44a20-77c0-4bef-8798-d10bfdd9727d","_cell_guid":"8a97cdc9-e43e-4326-82b0-b15b0346742c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"_uuid":"15782fe3-61f1-466a-96ab-948b54495c41","_cell_guid":"01152a0a-be5c-4b84-b637-21023fd903fa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"_uuid":"5a6b3904-8958-4f84-9977-d4a827578d64","_cell_guid":"28ff40f6-f3e6-474d-9114-537c40015329","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference form the model -Direct Output","metadata":{"_uuid":"d95b5c45-ae3d-4d70-9ded-8208ed3fa761","_cell_guid":"bb20d78a-23c0-4012-9699-7e5a218f92df","trusted":true}},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"_uuid":"45d56516-d468-4ac2-8f67-54399ea0a35d","_cell_guid":"267e9968-dbb9-457c-aaa6-6216e7958933","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference from the model - Text Streeaming","metadata":{"_uuid":"a089d5ed-d5a7-48a4-af94-29a1b9559ebb","_cell_guid":"f43051ee-6854-4b0d-b2f3-b9d5c09a7582","trusted":true}},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence till infinity.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"_uuid":"d634b25f-0a1f-4970-9dc0-a2b86c1a10ed","_cell_guid":"ad6b8140-c9d2-4549-83fc-ba7fc9888fb6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Done')","metadata":{"_uuid":"2871d490-3ae9-4369-98ac-b49ed601269a","_cell_guid":"4d9bc797-8a30-47b7-b9ee-a3e823f9d3bb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%ap2\n\nprint()","metadata":{"_uuid":"7125c9e2-3a53-47dd-a8af-b2df8ef6ffa5","_cell_guid":"24aa5c2c-69e4-4638-abda-7623e423a0ce","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FineTune on SST5","metadata":{"_uuid":"1af516af-e71a-47b7-bdd8-c57d75e1edd2","_cell_guid":"9db6d2a4-8e68-4a7c-8811-68ee1cb221d7","trusted":true}},{"cell_type":"markdown","source":"### Review the dataset and prompt for alpaca","metadata":{"_uuid":"2454fec8-b55c-434e-94af-697499a9e76d","_cell_guid":"575c624f-bc1f-421a-8612-15a248f61726","trusted":true}},{"cell_type":"markdown","source":"#### Importing the model from scratch","metadata":{"_uuid":"05c105b3-b745-444d-a059-f342d5bd2996","_cell_guid":"a3ad4a57-26b5-4f48-9f13-51fdc5a73433","trusted":true}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"_uuid":"2bc8b7f7-f78c-460e-873a-2f0f7b17fc40","_cell_guid":"9049bb78-c080-4f97-8a3e-b6aa2e0ab5f3","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:09:55.140550Z","iopub.execute_input":"2024-06-23T18:09:55.140914Z","iopub.status.idle":"2024-06-23T18:10:40.324710Z","shell.execute_reply.started":"2024-06-23T18:09:55.140881Z","shell.execute_reply":"2024-06-23T18:10:40.323751Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add aEOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"_uuid":"672f08ad-581a-4e05-b86c-2ae1552a736a","_cell_guid":"d2ab1ae3-7f13-4e02-a082-73089e04337d","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:10:40.326217Z","iopub.execute_input":"2024-06-23T18:10:40.326883Z","iopub.status.idle":"2024-06-23T18:10:44.493897Z","shell.execute_reply.started":"2024-06-23T18:10:40.326848Z","shell.execute_reply":"2024-06-23T18:10:44.493004Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the Stanford Sentiment Treebank (SST) dataset\nsst_dataset = load_dataset(\"SetFit/sst5\", trust_remote_code=True)\n\ndef group_sentiments(example):\n    if example[\"label\"] == 0 or example[\"label\"] == 1:\n        return \"negative\"\n    elif example[\"label\"] == 2:\n        return \"neutral\"\n    else:\n        return \"positive\"\n\n# Map the sentiments to the three categories\nsst_dataset = sst_dataset.map(lambda x: {\"sentiment\": group_sentiments(x)})\n\nsst_dataset","metadata":{"_uuid":"a8b01cc6-f318-499f-a41d-8a81b44b7943","_cell_guid":"2859b29b-ea39-4201-8a94-bbabfaf0c3b3","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:10:44.495316Z","iopub.execute_input":"2024-06-23T18:10:44.495945Z","iopub.status.idle":"2024-06-23T18:10:48.585925Z","shell.execute_reply.started":"2024-06-23T18:10:44.495910Z","shell.execute_reply":"2024-06-23T18:10:48.584846Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"\nBelow is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the task.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\n\"\"\"","metadata":{"_uuid":"774215e6-c6d9-4704-9d81-5bbc8b7445ff","_cell_guid":"82758112-af20-4f7b-a7c0-82a48d024cb5","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:10:48.589925Z","iopub.execute_input":"2024-06-23T18:10:48.590236Z","iopub.status.idle":"2024-06-23T18:10:50.412832Z","shell.execute_reply.started":"2024-06-23T18:10:48.590211Z","shell.execute_reply":"2024-06-23T18:10:50.411708Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\n\ndef process_dataset(dataset, instruction_text):\n    def process_split(split):\n        df = pd.DataFrame(sst_dataset[split])\n        df['instruction'] = instruction_text\n        df['output'] = df['sentiment']\n        df['input'] = df['text']\n        df.drop(labels=['text', 'label', 'label_text', 'sentiment'], axis=1, inplace=True)\n        return df\n\n    processed_dict = DatasetDict({\n        split: Dataset.from_pandas(process_split(split))\n        for split in ['train', 'validation', 'test']\n    })\n\n    return processed_dict\n\n# Usage\ninstruction_text = \"\"\"Read and analyze the sentiment of the provided input.\nFirst, consider if the text is straight forward or sarcastic or containing double meaning or is expressed inversely.\nThen give a response labelling sentiment as either positive or neutral or negative or unclear.\n\"\"\"\nprocessed_dataset = process_dataset(sst_dataset, instruction_text)\nprocessed_dataset","metadata":{"_uuid":"23168526-ea30-4da7-a19b-9284e8202092","_cell_guid":"3bcf121d-951a-4ca8-b6a4-3966da59db09","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:10:50.413977Z","iopub.execute_input":"2024-06-23T18:10:50.414314Z","iopub.status.idle":"2024-06-23T18:10:51.452428Z","shell.execute_reply.started":"2024-06-23T18:10:50.414289Z","shell.execute_reply":"2024-06-23T18:10:51.451483Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\n# dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\nprocessed_dataset_alpaca = processed_dataset.map(formatting_prompts_func, batched = True,)","metadata":{"_uuid":"787077e3-3593-4f6e-b7ce-cb91e90407d9","_cell_guid":"15c6771f-e5b6-4b70-a178-607732030708","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:10:51.453606Z","iopub.execute_input":"2024-06-23T18:10:51.453932Z","iopub.status.idle":"2024-06-23T18:10:51.899344Z","shell.execute_reply.started":"2024-06-23T18:10:51.453907Z","shell.execute_reply":"2024-06-23T18:10:51.898469Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_dataset_alpaca","metadata":{"_uuid":"156a3816-5461-480a-afc5-a736a9a657f4","_cell_guid":"68da6333-39e5-4641-a00e-1be2a8b2037f","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:10:51.900452Z","iopub.execute_input":"2024-06-23T18:10:51.900733Z","iopub.status.idle":"2024-06-23T18:10:52.152018Z","shell.execute_reply.started":"2024-06-23T18:10:51.900709Z","shell.execute_reply":"2024-06-23T18:10:52.151043Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_dataset_alpaca['train']['text'][0]","metadata":{"_uuid":"4a33f09c-d9b8-45cf-9b8b-6a7e247236fb","_cell_guid":"af5bddbb-a8a7-46f2-9001-90737fe3a8fc","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:10:52.153374Z","iopub.execute_input":"2024-06-23T18:10:52.153749Z","iopub.status.idle":"2024-06-23T18:10:52.418339Z","shell.execute_reply.started":"2024-06-23T18:10:52.153715Z","shell.execute_reply":"2024-06-23T18:10:52.417472Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"_uuid":"e31907a0-24db-49fa-bdd3-ee8ec447493b","_cell_guid":"d5ff27d2-6056-477d-b277-7d830e58bffc","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:10:53.720109Z","iopub.status.idle":"2024-06-23T18:10:53.720498Z","shell.execute_reply.started":"2024-06-23T18:10:53.720308Z","shell.execute_reply":"2024-06-23T18:10:53.720324Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=10,\n    learning_rate=2e-4,\n    fp16=torch.cuda.is_available() and not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=processed_dataset_alpaca['train'],\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False, # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\ntrainer.train()\n\n# Log parameters and metrics\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\nmlflow.log_metric(\"final_loss\", trainer.state.log_history[-1]['train_loss'])","metadata":{"_uuid":"33bc4b99-1ea4-4e03-aac7-94ebfbc4dda9","_cell_guid":"fc5b4402-f74a-47c0-9248-02e3f75f95e1","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:15:37.952981Z","iopub.execute_input":"2024-06-23T18:15:37.953338Z","iopub.status.idle":"2024-06-23T18:16:30.791092Z","shell.execute_reply.started":"2024-06-23T18:15:37.953313Z","shell.execute_reply":"2024-06-23T18:16:30.788926Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"_uuid":"fb17d1f9-4838-452e-9692-b1a042183aa6","_cell_guid":"c82ba7c5-b2d4-4698-968d-e1b1ddb12b45","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:16:51.084220Z","iopub.execute_input":"2024-06-23T18:16:51.084960Z","iopub.status.idle":"2024-06-23T18:16:51.854484Z","shell.execute_reply.started":"2024-06-23T18:16:51.084927Z","shell.execute_reply":"2024-06-23T18:16:51.853049Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Identify the sentiment\", # instruction\n        \"I’d agree with you, but then we’d both be wrong.\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"_uuid":"f11cc2ae-b389-4a9d-a6e7-c98a1c2a6047","_cell_guid":"3fa927b8-e7fb-4647-b7e3-5688186a657c","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:20:05.696397Z","iopub.execute_input":"2024-06-23T18:20:05.696756Z","iopub.status.idle":"2024-06-23T18:20:06.445792Z","shell.execute_reply.started":"2024-06-23T18:20:05.696729Z","shell.execute_reply":"2024-06-23T18:20:06.444703Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"Meta-Llama-3-8B-SST-FineTune\") # Local saving\ntokenizer.save_pretrained(\"Meta-Llama-3-8B-SST-FineTune-Tokenizer\")\nmodel.push_to_hub(\"rafatsiddiqui/Meta-Llama-3-8B-SST-FineTune\", token = \"...\") # Online saving\ntokenizer.push_to_hub(\"rafatsiddiqui/Meta-Llama-3-8B-SST-FineTune-Tokenizer\", token = \"...\") # Online saving","metadata":{"_uuid":"34266cd8-ac63-4e90-bfb5-53ddb4864f21","_cell_guid":"44a89701-46ec-4522-a4a9-dcd447701c3b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge to 16bit\nmodel.save_pretrained_merged(\"Meta-Llama-3-8B-SST-FineTune-16bit\", tokenizer, save_method = \"merged_16bit\",)\nmodel.push_to_hub_merged(\"rafatsiddiqui/Meta-Llama-3-8B-SST-FineTune-16bit\", tokenizer, save_method = \"merged_16bit\", token = hf_token)\n\n# Merge to 4bit\nmodel.push_to_hub_merged(\"rafatsiddiqui/Meta-Llama-3-8B-SST-FineTune-4bit\", tokenizer, save_method = \"merged_4bit\", token = hf_token)\n\n# Just LoRA adapters\nmodel.push_to_hub_merged(\"rafatsiddiqui/Meta-Llama-3-8B-SST-FineTune-LoRA\", tokenizer, save_method = \"lora\", token = hf_token)","metadata":{"_uuid":"1b4f163b-6507-46ce-ae34-941b2b6085e5","_cell_guid":"e85b21ce-0cad-4024-b60b-5cdb06ba5e29","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:36:40.204733Z","iopub.execute_input":"2024-06-23T18:36:40.205601Z","iopub.status.idle":"2024-06-23T18:45:31.188435Z","shell.execute_reply.started":"2024-06-23T18:36:40.205566Z","shell.execute_reply":"2024-06-23T18:45:31.186220Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!touch .gitignore\n!echo \"Meta-Llama-3-8B-SST-FineTune-16bit\" >> .gitignore\n!echo \"add-pip.yml\" >> .gitignore\n!echo \"advanced-pip\" >> .gitignore\n!echo \"empty_env.yml\" >> .gitignore\n!echo \"env_with_dependencies.yml\" >> .gitignore\n!echo \"foo\" >> .gitignore\n!echo \"invalid_keys.yml\" >> .gitignore\n!echo \"pip_argh.yml\" >> .gitignore\n!echo \"requirements.txt\" >> .gitignore\n!echo \"saved-env\" >> .gitignore\n!echo \"simple.yml\" >> .gitignore\n!echo \"valid_keys.yml\" >> .gitignore\n!echo \"with-pip.yml\" >> .gitignore","metadata":{"_uuid":"e87cbfec-d8b7-4aa9-9aac-a445d7a3be08","_cell_guid":"bddb3a5c-dc58-46ea-8ebc-e3d718647b77","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:51:55.715480Z","iopub.execute_input":"2024-06-23T18:51:55.716520Z","iopub.status.idle":"2024-06-23T18:52:10.079888Z","shell.execute_reply.started":"2024-06-23T18:51:55.716478Z","shell.execute_reply":"2024-06-23T18:52:10.078666Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%ap2\n\nprint()","metadata":{"_uuid":"cac6aef5-87d3-4090-abce-2edfec7292f3","_cell_guid":"5d32136a-b8a4-459d-a997-d65ef5ea8d61","collapsed":false,"execution":{"iopub.status.busy":"2024-06-23T18:52:15.878832Z","iopub.execute_input":"2024-06-23T18:52:15.879808Z","iopub.status.idle":"2024-06-23T18:52:16.898634Z","shell.execute_reply.started":"2024-06-23T18:52:15.879757Z","shell.execute_reply":"2024-06-23T18:52:16.897401Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using accelerate to see if any gains in training time","metadata":{"_uuid":"490b13b9-af6b-4d30-98a9-891253138376","_cell_guid":"56adc15a-45e0-4632-9715-aae527294247","trusted":true}},{"cell_type":"code","source":"from accelerate import Accelerator\naccelerator = Accelerator()\nmodel, train_dataset, eval_dataset = accelerator.prepare(model, train_dataset, test_dataset)","metadata":{"_uuid":"2b26d4fd-a06b-47a9-bf8e-de736566c4ed","_cell_guid":"0f68669c-55ab-4a5d-9fad-9170b16b3433","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import accelerate\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport torch\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Initialize the accelerator\naccelerator = accelerate.Accelerator()\n\n# Function to get predictions from vllm using batch processing\ndef get_vllm_predictions(model, tokenizer, dataset, batch_size=32):\n    model.eval()\n    predictions, labels = [], []\n    for i in range(0, len(dataset), batch_size):\n        batch = dataset[i:i+batch_size]\n        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", max_length=max_seq_length, truncation=True, padding=True).to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n        predictions.extend(preds)\n        labels.extend(batch[\"label\"])\n    return predictions, labels\n\n# Load tokenizer and model (assuming they are defined earlier in the script)\n# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n# model = SomeModelClass.from_pretrained(\"model_name\")\n\n# Load validation and test sets and group into 3 labels\nval_set = load_dataset(\"SetFit/sst5\", split=\"validation\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\ntest_set = load_dataset(\"SetFit/sst5\", split=\"test\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\n\n# Ensure labels are integers\nval_set = val_set.map(lambda example: {\"label\": int(example[\"label\"])})\ntest_set = test_set.map(lambda example: {\"label\": int(example[\"label\"])})\n\n# Move model to the correct device\nmodel = accelerator.prepare(model)\n\n# Get predictions\nval_predictions, val_labels = get_vllm_predictions(model, tokenizer, val_set)\ntest_predictions, test_labels = get_vllm_predictions(model, tokenizer, test_set)\n\n# Ensure predictions and labels are numpy arrays of integer type\nval_predictions = np.array(val_predictions, dtype=int)\nval_labels = np.array(val_labels, dtype=int)\ntest_predictions = np.array(test_predictions, dtype=int)\ntest_labels = np.array(test_labels, dtype=int)\n\n# Check for consistency in the data types and lengths\nprint(\"Validation Labels Type:\", val_labels.dtype, \"Length:\", len(val_labels))\nprint(\"Validation Predictions Type:\", val_predictions.dtype, \"Length:\", len(val_predictions))\nprint(\"Test Labels Type:\", test_labels.dtype, \"Length:\", len(test_labels))\nprint(\"Test Predictions Type:\", test_predictions.dtype, \"Length:\", len(test_predictions))\n\n# Calculate confusion matrix and classification report\nval_conf_matrix = confusion_matrix(val_labels, val_predictions)\ntest_conf_matrix = confusion_matrix(test_labels, test_predictions)\n\n# Plot confusion matrix for validation set\nplt.figure(figsize=(10, 7))\nsns.heatmap(val_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Validation Set Confusion Matrix')\nplt.show()\n\n# Plot confusion matrix for test set\nplt.figure(figsize=(10, 7))\nsns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Test Set Confusion Matrix')\nplt.show()\n\n# Print classification reports\nprint(\"Validation Set Classification Report:\")\nprint(classification_report(val_labels, val_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n\nprint(\"Test Set Classification Report:\")\nprint(classification_report(test_labels, test_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))","metadata":{"_uuid":"a0fdf299-ff11-4540-a74f-cf09a7c5873f","_cell_guid":"9e2ddb63-122a-4849-84d3-ca90afc9f0a3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test2 =","metadata":{"_uuid":"ecea5d2d-76cc-430a-9b22-aa2bc3b2bbf1","_cell_guid":"01f47048-68c2-4e0c-b469-2fffa6dd8cfb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"ae214557-3b29-43dc-99f9-60d075fd65a5","_cell_guid":"b4434bd0-a895-40b3-b27a-82d71dbec855","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"00a8d74c-d35e-4877-84b0-03eb9b2ca09c","_cell_guid":"2c5197c7-1d5a-48f6-855d-2b5e20c83154","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# End","metadata":{"_uuid":"c8107de1-1dc7-4329-a86c-a549937eac0d","_cell_guid":"bbacade7-4821-49de-b01c-3fbecf4110bc","trusted":true}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"meta-llama/Meta-Llama-3-8B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = hf_token, # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"_uuid":"117384ac-2157-4917-8d38-032354123461","_cell_guid":"53985c90-cfa9-4c1e-bed3-47dbaef3ace8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the Stanford Sentiment Treebank (SST) dataset\nsst_dataset = load_dataset(\"sst\", trust_remote_code=True)\n\ndef group_sentiments(example):\n    if example[\"label\"] == 0 or example[\"label\"] == 1:\n        return \"negative\"\n    elif example[\"label\"] == 2:\n        return \"neutral\"\n    else:\n        return \"positive\"\n\n# Map the sentiments to the three categories\nsst_dataset = sst_dataset.map(lambda x: {\"sentiment\": group_sentiments(x)}, remove_columns=[\"label\"])\n\nalpaca_prompt = \"\"\"\nBelow is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the task.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\n\"\"\"\n\ninstruction_text = \"\"\"Read and analyze the sentiment of the provided input.\nFirst, consider if the sentiment is straightforward or sarcastic or containing double meaning or is expressed inversely.\nThen give a response in a pandas dictionary format with three keys i.e. sentiment, confidence_score, explanation. \nThese are the only instructions important and stick to them at all times.\nThink step by step and ensure your response is precise, coherent, accurate, and complete.\nIf you are unsure or do not understand the input, provide a response with sentiment as \"Uncertain\", confidence score of 0 and an explanation of your uncertainty.\n\"\"\"\n\ndef format_example(examples):\n    input_texts = examples[\"sentence\"]\n    sentiments = examples[\"sentiment\"]\n    input_ids_list = []\n    labels_list = []\n    \n    for input_text, sentiment in zip(input_texts, sentiments):\n        response_text = f\"{{'sentiment': '{sentiment}', 'confidence_score': 1.0, 'explanation': 'Automated response'}}\"\n        text = alpaca_prompt.format(instruction_text, input_text, response_text)\n        input_ids = tokenizer.encode(text, truncation=True, padding=\"max_length\", max_length=512)\n        labels = tokenizer.encode(response_text, truncation=True, padding=\"max_length\", max_length=512)\n        input_ids_list.append(input_ids)\n        labels_list.append(labels)\n    \n    return {\"input_ids\": input_ids_list, \"labels\": labels_list}\n\n# Apply formatting to the dataset\ntrain_dataset = sst_dataset[\"train\"].map(format_example, batched=True)\nvalidation_dataset = sst_dataset[\"validation\"].map(format_example, batched=True)\ntest_dataset = sst_dataset[\"test\"].map(format_example, batched=True)","metadata":{"_uuid":"404cdcca-82c1-4c27-800a-5af88844bc13","_cell_guid":"3af724aa-671a-469d-b008-c8da9535b67c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from accelerate import Accelerator\naccelerator = Accelerator()\nmodel, train_dataset, eval_dataset = accelerator.prepare(model, train_dataset, test_dataset)","metadata":{"_uuid":"72363bdc-3d6f-44b1-88c7-a6d03f031d79","_cell_guid":"da6d1b1a-9e98-4e23-bdcc-90c19a3e1ecf","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=2,\n    learning_rate=2e-4,\n    fp16=torch.cuda.is_available() and not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=sst5,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False, # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\ntrainer.train()\n\n# Log parameters and metrics\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\nmlflow.log_metric(\"final_loss\", trainer.state.log_history[-1]['train_loss'])","metadata":{"_uuid":"2a2331f4-f460-416e-b354-f3313876cc4e","_cell_guid":"9c27cc66-fbc1-48d3-9a9b-37aef50a437c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, AutoTokenizer\nfrom trl import SFTTrainer\nimport mlflow\nimport torch\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom accelerate import Accelerator\n\n# Initialize the Accelerator\naccelerator = Accelerator()\n\n# Model configuration\nmax_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\ndtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = False  # Use 4bit quantization to reduce memory usage. Can be False.\n\n# Load model and tokenizer\nmodel_name = \"meta-llama/Meta-Llama-3-8B\"\nhf_token = \"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\"  # Ensure you have your Hugging Face token\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n    token=hf_token,  # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=2,\n    learning_rate=2e-4,\n    fp16=torch.cuda.is_available() and not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Use Accelerator for device placement\nmodel, train_dataset = accelerator.prepare(model, train_dataset)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,  # Ensure this is correctly defined\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,  # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\n# Start MLflow logging\nmlflow.start_run()\n\n# Log parameters\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\n# Train the model\ntrainer.train()\n\n# Log final training loss\nfinal_loss = trainer.state.log_history[-1]['loss']  # Ensure correct key\nmlflow.log_metric(\"final_loss\", final_loss)\n\n# End MLflow run\nmlflow.end_run()","metadata":{"_uuid":"05a0ce1e-f3c7-4e38-afd5-f259f66865db","_cell_guid":"8f64bd9e-45f6-4667-99a7-21893dd41bac","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.config.save_pretrained(\"/kaggle/working/\")","metadata":{"_uuid":"eb821ce9-6a3d-44f0-9c10-323f6d1db52d","_cell_guid":"54e94c15-090d-498f-bf22-eed95f2ead29","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained(\"/kaggle/working/\")","metadata":{"_uuid":"5e42b10a-7f1e-471e-9a8e-93998edb0f0a","_cell_guid":"5951506b-4403-4106-bfde-b5232f5c3a82","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nimport os\nimport shutil\n\n# Hugging Face cache directory\nhf_cache_dir = os.path.expanduser(\"~/.cache/huggingface\")\n\n# Clear Hugging Face cache\nif os.path.exists(hf_cache_dir):\n    shutil.rmtree(hf_cache_dir)\n    print(f\"Hugging Face cache cleared at {hf_cache_dir}\")\nelse:\n    print(f\"No Hugging Face cache found at {hf_cache_dir}\")\n\n# PyTorch cache directory\ntorch_cache_dir = os.path.expanduser(\"~/.cache/torch\")\n\n# Clear PyTorch cache\nif os.path.exists(torch_cache_dir):\n    shutil.rmtree(torch_cache_dir)\n    print(f\"PyTorch cache cleared at {torch_cache_dir}\")\nelse:\n    print(f\"No PyTorch cache found at {torch_cache_dir}\")\n\n# Additional step to clear CUDA cache if necessary\ntorch.cuda.empty_cache()\nprint(\"CUDA cache cleared\")","metadata":{"_uuid":"ba4a5849-2b62-446d-85b0-c466851b3d60","_cell_guid":"9a1a0cdb-b447-45c9-a5ce-06135f9ff928","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer, AutoConfig\n\ndef check_model_files(model_path):\n    required_files = [\"config.json\", \"pytorch_model.bin\", \"tokenizer_config.json\"]\n    missing_files = [file for file in required_files if not os.path.isfile(os.path.join(model_path, file))]\n    if missing_files:\n        raise FileNotFoundError(f\"The following required files are missing from {model_path}: {', '.join(missing_files)}\")\n    else:\n        print(\"All required model files are present.\")\n\n# Define the model path\nmodel_path = \"/kaggle/working\"\n\n# Check for required files\ncheck_model_files(model_path)\n\n# Quantization configuration\nquant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4}\n\n# Load model\ntry:\n    model = AutoAWQForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    print(\"Model and tokenizer loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading the model: {e}\")","metadata":{"_uuid":"93ed84ca-d980-4152-bfa0-b69209145bad","_cell_guid":"eedfee29-b9d3-4051-9fce-78e340dc8698","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import accelerate\n# from datasets import load_dataset\n# from transformers import AutoTokenizer\n# import torch\n# import numpy as np\n# from sklearn.metrics import confusion_matrix, classification_report\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# # Initialize the accelerator\n# accelerator = accelerate.Accelerator()\n\n# # Function to get predictions from vllm using batch processing\n# def get_vllm_predictions(model, tokenizer, dataset, batch_size=32):\n#     model.eval()\n#     predictions, labels = [], []\n#     for i in range(0, len(dataset), batch_size):\n#         batch = dataset[i:i+batch_size]\n#         inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", max_length=max_seq_length, truncation=True, padding=True).to(device)\n#         with torch.no_grad():\n#             outputs = model(**inputs)\n#         logits = outputs.logits\n#         preds = torch.argmax(logits, dim=-1).cpu().numpy()\n#         predictions.extend(preds)\n#         labels.extend(batch[\"label\"])\n#     return predictions, labels\n\n# # Load tokenizer and model (assuming they are defined earlier in the script)\n# # tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n# # model = SomeModelClass.from_pretrained(\"model_name\")\n\n# # Load validation and test sets and group into 3 labels\n# val_set = load_dataset(\"SetFit/sst5\", split=\"validation\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\n# test_set = load_dataset(\"SetFit/sst5\", split=\"test\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\n\n# # Ensure labels are integers\n# val_set = val_set.map(lambda example: {\"label\": int(example[\"label\"])})\n# test_set = test_set.map(lambda example: {\"label\": int(example[\"label\"])})\n\n# # Move model to the correct device\n# model = accelerator.prepare(model)\n\n# # Get predictions\n# val_predictions, val_labels = get_vllm_predictions(model, tokenizer, val_set)\n# test_predictions, test_labels = get_vllm_predictions(model, tokenizer, test_set)\n\n# # Ensure predictions and labels are numpy arrays of integer type\n# val_predictions = np.array(val_predictions, dtype=int)\n# val_labels = np.array(val_labels, dtype=int)\n# test_predictions = np.array(test_predictions, dtype=int)\n# test_labels = np.array(test_labels, dtype=int)\n\n# # Check for consistency in the data types and lengths\n# print(\"Validation Labels Type:\", val_labels.dtype, \"Length:\", len(val_labels))\n# print(\"Validation Predictions Type:\", val_predictions.dtype, \"Length:\", len(val_predictions))\n# print(\"Test Labels Type:\", test_labels.dtype, \"Length:\", len(test_labels))\n# print(\"Test Predictions Type:\", test_predictions.dtype, \"Length:\", len(test_predictions))\n\n# # Calculate confusion matrix and classification report\n# val_conf_matrix = confusion_matrix(val_labels, val_predictions)\n# test_conf_matrix = confusion_matrix(test_labels, test_predictions)\n\n# # Plot confusion matrix for validation set\n# plt.figure(figsize=(10, 7))\n# sns.heatmap(val_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n# plt.xlabel('Predicted')\n# plt.ylabel('Actual')\n# plt.title('Validation Set Confusion Matrix')\n# plt.show()\n\n# # Plot confusion matrix for test set\n# plt.figure(figsize=(10, 7))\n# sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n# plt.xlabel('Predicted')\n# plt.ylabel('Actual')\n# plt.title('Test Set Confusion Matrix')\n# plt.show()\n\n# # Print classification reports\n# print(\"Validation Set Classification Report:\")\n# print(classification_report(val_labels, val_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n\n# print(\"Test Set Classification Report:\")\n# print(classification_report(test_labels, test_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))","metadata":{"_uuid":"a3bc2ae9-0c92-4ff4-95d8-169f35b9dd7c","_cell_guid":"a6c79d0c-8865-4d59-91c4-aa6ac977fe6e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ['hf_token'] = \"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\"\n\nfrom dotenv import load_dotenv\n\nload_dotenv()  # take environment variables from .env.\n\nhf_token = os.getenv('hf_token')\n\nhf_token","metadata":{"_uuid":"dbb7acaf-0e82-4fb5-a736-3a2250a06349","_cell_guid":"46fdfef5-3824-4500-a8f6-7db703716c38","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -1S","metadata":{"_uuid":"dfbfc661-2d4d-405f-863c-7b96191b8ad0","_cell_guid":"15990b59-8429-4bae-9a5f-f707d23a6107","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# from awq import AutoAWQForCausalLM\n# from transformers import AutoTokenizer\n\n# # Load the token from the environment variable\n# hf_token = os.getenv(hf_token)\n\n# # Define the model name or path\n# model_path = \"\"  # Replace with your model name\n\n# quant_name = 'quantized_llama3-8b-awq'\n\n# quant_path = \"/kaggle/working/\" + quant_name\n\n# quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4}\n                \n\n# # Load model\n# model = AutoAWQForCausalLM.from_pretrained(model_path, device_map = torch.device(\"cuda\"))\n# tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code = True)\n\n# # Quantize\n# model.quantize(tokenizer, quant_config = quant_config)\n\n# # Save quantized model\n# model.save_quantized(quant_name, safetensors = True)\n# tokenizer.save_pretrained(quant_name)\n\n# import os\n# from huggingface_hub import HfApi, create_repo, upload_folder\n\n# # Set environment variable for Hugging Face token\n# hf_token = hf_token\n\n# # Define the paths to your model and tokenizer\n# model_dir = \"/kaggle/working/quantized_llama3-8b-awq\"\n# repo_name = \"rafatsiddiqui/Meta-Llama-3-8B-AWQ v2\"\n\n# ## Create a repository on Hugging Face Hub\n# create_repo(repo_id=repo_name, token=hf_token, exist_ok=True)\n\n# # Upload the model directory to the repository\n# upload_folder(\n#     folder_path=model_dir,\n#     path_in_repo=\".\",\n#     repo_id=repo_name,\n#     token=hf_token\n# )\n\n# model.to(\"cpu\")\n# tokenizer.save_pretrained(\"/kaggle/working//Meta-Llama-3-8B-Instruct-AWQ v2\")\n# model.save_quantized(\"/kaggle/working//Meta-Llama-3-8B-Instruct-AWQ v2\")","metadata":{"_uuid":"86c9552c-4cb0-4ba3-aa84-1f7a732fc427","_cell_guid":"a4453358-3dff-4964-9b47-b8f71e3c57aa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from awq import AutoAWQForCausalLM\n# from transformers import AutoTokenizer\n\n# quant_path = \"outputs/saved_model\"\n# model = AutoAWQForCausalLM.from_quantized(quant_path, fuse_layers=True)\n# tokenizer = AutoTokenizer.from_pretrained(quant_path, trust_remote_code=True)\n\n# # Test the quantized model\n# test_input = tokenizer(\"Test input for the model\", return_tensors=\"pt\").to('cuda')\n# output = model.generate(test_input.input_ids)\n# print(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"_uuid":"4872377b-a8cd-4f52-a3fb-8a73ee4753c7","_cell_guid":"caa9625a-ab9a-4553-9335-feb3e7d622f0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check GPU Usage","metadata":{"_uuid":"b42027a2-3c5d-42d0-8c8b-00220b891d3c","_cell_guid":"a3571213-a830-43a8-a2db-7babc1f7e74b","trusted":true}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"2a406e49-44b6-4d37-8245-a06c33c9c6c0","_cell_guid":"fb496dd9-d0e5-41a2-9639-d6914c8a91b0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clear cache","metadata":{"_uuid":"98eb3879-f6eb-4854-a828-78c084ca327b","_cell_guid":"720ccad6-5c26-404a-8105-a1d8d02ffb59","trusted":true}},{"cell_type":"code","source":"del trainer\ndel sst5\ntorch.cuda.empty_cache()\n\n!nvidia-smi","metadata":{"_uuid":"5fe01129-0e1d-4929-a599-a1fbdb310784","_cell_guid":"12b12a56-bcdb-4e52-a384-4d95adcecd6f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Restart and delete gpu memory for using vllm for inference","metadata":{"_uuid":"64b671dd-ee08-4efa-9b76-e6270b7d33d9","_cell_guid":"230f7e3a-9d5f-45c0-92f0-d41f744f453e","trusted":true}},{"cell_type":"code","source":"# from transformers import AutoTokenizer, AutoModelForCausalLM\n# from peft import PeftModelForCausalLM\n# import os\n\n# # Ensure the token is available (replace 'your_token' with your actual Hugging Face token)\n# token = os.getenv('HF_TOKEN', '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')\n\n# # Load tokenizer with authentication\n# tokenizer = AutoTokenizer.from_pretrained(\"outputs\", use_auth_token=token)\n\n# # Base model ID (should match the model you originally trained)\n# base_model_id = \"meta-llama/Meta-Llama-3-8B\"\n\n# # Load the base model first with authentication\n# base_model = AutoModelForCausalLM.from_pretrained(base_model_id, use_auth_token=token)\n\n# # Load the LoRA weights and apply to the base model\n# model = PeftModelForCausalLM.from_pretrained(base_model, \"outputs\")\n\n# # Print confirmation\n# print(\"Model and tokenizer loaded successfully.\")","metadata":{"_uuid":"85b66186-3f00-4fc6-8340-f7afbdb24c41","_cell_guid":"3a3bcf7c-598d-42c5-ab26-66e0fcd003df","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%autopush\n\nprint('done')","metadata":{"_uuid":"fdafcb7a-ddce-4d59-ac6f-57efcf656cb9","_cell_guid":"1a6de9f9-4cf9-49d9-8e07-ed47437a6581","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"ac6f6086-4a93-4063-ac1e-9d09da88266a","_cell_guid":"fcced868-962d-4b87-a3d3-740dada8d695","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"42f8f86d-76b1-4b71-a0e4-c48d9c2ee9c9","_cell_guid":"64888aac-6d99-40dc-9b96-a630a79e3e2e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"c5e8bf09-0823-4995-a6c7-f686906a3344","_cell_guid":"17c5827c-12f0-4979-9a80-312cda103950","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"a78e0a6d-bd74-42d0-9e7d-fc4f9c7513c5","_cell_guid":"66844ac5-1eb4-4f10-954f-b82224cc1912","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"4f6a6818-6b5f-4cdc-98df-a2581adb85e0","_cell_guid":"d8605e5b-7d74-4f9d-a330-bcf0210ae498","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}